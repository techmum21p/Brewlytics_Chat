{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modeling_intro",
   "metadata": {},
   "source": [
    "# Modeling: Offer Completion Prediction\n",
    "\n",
    "**Goal:** Build and compare ML models to predict which customers will complete offers.\n",
    "\n",
    "**Models to Train:**\n",
    "1. Logistic Regression (baseline)\n",
    "2. Decision Tree (baseline)\n",
    "3. Random Forest (ensemble)\n",
    "4. XGBoost (ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready! ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            classification_report, roc_curve, precision_recall_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Environment ready! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_processed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LOADED\n",
      "============================================================\n",
      "\n",
      "Training set: 69,145 samples √ó 26 features\n",
      "Test set: 17,287 samples √ó 26 features\n",
      "\n",
      "Target distribution in train set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "processed_dir = '../Cafe_Rewards_Offers/processed'\n",
    "\n",
    "X_train = joblib.load(f'{processed_dir}/X_train_scaled.pkl')\n",
    "X_test = joblib.load(f'{processed_dir}/X_test_scaled.pkl')\n",
    "y_train = joblib.load(f'{processed_dir}/y_train.pkl')\n",
    "y_test = joblib.load(f'{processed_dir}/y_test.pkl')\n",
    "feature_names = joblib.load(f'{processed_dir}/feature_names.pkl')\n",
    "scaler = joblib.load(f'{processed_dir}/scaler.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining set: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "print(f\"\\nTarget distribution in train set:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check_data_leakage",
   "metadata": {},
   "source": [
    "## üîç Check for Data Leakage\n",
    "\n",
    "**Important:** Perfect metrics (1.0 accuracy) usually indicate data leakage.\n",
    "Let's check if any features directly reveal the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check_leakage_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR DATA LEAKAGE\n",
      "============================================================\n",
      "\n",
      "Features (26 total):\n",
      "   0. received_time\n",
      "   1. difficulty\n",
      "   2. duration\n",
      "   3. in_email\n",
      "   4. in_mobile\n",
      "   5. in_social\n",
      "   6. in_web\n",
      "   7. offer_received\n",
      "   8. offer_viewed\n",
      "   9. offer_completed\n",
      "  10. age\n",
      "  11. income\n",
      "  12. membership_year\n",
      "  13. is_demographics_missing\n",
      "  14. membership_duration_days\n",
      "  15. membership_month\n",
      "  16. offer_type_bogo\n",
      "  17. offer_type_discount\n",
      "  18. offer_type_informational\n",
      "  19. gender_F\n",
      "  20. gender_M\n",
      "  21. gender_Missing\n",
      "  22. gender_O\n",
      "  23. age_group_encoded\n",
      "  24. income_bracket_encoded\n",
      "  25. tenure_group_encoded\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR PERFECT CORRELATION\n",
      "============================================================\n",
      "\n",
      "Top correlations with target:\n",
      "  target                        : 1.0000\n",
      "  offer_completed               : 1.0000\n",
      "  duration                      : 0.3518\n",
      "  income                        : 0.3164\n",
      "  income_bracket_encoded        : 0.3081\n",
      "  difficulty                    : 0.2695\n",
      "  offer_type_discount           : 0.2497\n",
      "  tenure_group_encoded          : 0.2294\n",
      "  in_web                        : 0.2149\n",
      "  gender_F                      : 0.1899\n",
      "\n",
      "‚ö†Ô∏è  DATA LEAKAGE DETECTED!\n",
      "Features with perfect correlation (r=1.0):\n",
      "  - target\n",
      "  - offer_completed\n",
      "\n",
      "‚ö†Ô∏è  ACTION REQUIRED: Remove these features before modeling!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR DATA LEAKAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check feature names\n",
    "print(f\"\\nFeatures ({len(feature_names)} total):\")\n",
    "for i, feat in enumerate(feature_names):\n",
    "    print(f\"  {i:2}. {feat}\")\n",
    "\n",
    "# Check if target has perfect correlation with any feature\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING FOR PERFECT CORRELATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine X and y for correlation check\n",
    "train_df = X_train.copy()\n",
    "train_df['target'] = y_train.values\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = train_df.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop correlations with target:\")\n",
    "for feat, corr in correlations.head(10).items():\n",
    "    print(f\"  {feat:30}: {corr:.4f}\")\n",
    "\n",
    "# Flag potential data leaks (correlation = 1.0 or near 1.0)\n",
    "perfect_leaks = correlations[correlations == 1.0]\n",
    "if len(perfect_leaks) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  DATA LEAKAGE DETECTED!\")\n",
    "    print(f\"Features with perfect correlation (r=1.0):\")\n",
    "    for feat in perfect_leaks.index:\n",
    "        print(f\"  - {feat}\")\n",
    "    print(\"\\n‚ö†Ô∏è  ACTION REQUIRED: Remove these features before modeling!\")\n",
    "else:\n",
    "    print(\"\\n‚úì No perfect data leaks detected (correlation < 1.0)\")\n",
    "\n",
    "# Check for near-perfect leaks (correlation > 0.95)\n",
    "near_leaks = correlations[(correlations > 0.95) & (correlations < 1.0)]\n",
    "if len(near_leaks) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  NEAR-PERFECT DATA LEAKAGE DETECTED!\")\n",
    "    print(f\"Features with near-perfect correlation (r > 0.95):\")\n",
    "    for feat, corr in near_leaks.items():\n",
    "        print(f\"  - {feat:30}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944933ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR DATA LEAKAGE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  DATA LEAKAGE DETECTED!\n",
      "============================================================\n",
      "Column 'offer_completed' found in features.\n",
      "This is IDENTICAL to target, causing perfect 1.0 predictions.\n",
      "Dropping 'offer_completed' from train and test sets...\n",
      "‚úì Dropped. New train shape: (69145, 25)\n",
      "‚úì Features remaining: 25\n",
      "\n",
      "‚ÑπÔ∏è  INFO: 'offer_viewed' feature present\n",
      "============================================================\n",
      "This feature is a potential data leak.\n",
      "It's available before completion in real-time scenarios.\n",
      "For now, we'll keep it to see full model performance.\n",
      "============================================================\n",
      "\n",
      "RECOMMENDATION:\n",
      "For true real-time prediction models, consider:\n",
      "  1. Train models WITHOUT 'offer_viewed'\n",
      "  2. For 'post-notification' prediction, keep 'offer_viewed'\n",
      "\n",
      "For now, we'll keep it to see full model performance.\n",
      "\n",
      "============================================================\n",
      "FINAL DATA SHAPE\n",
      "============================================================\n",
      "Train: 69,145 samples √ó 25 features\n",
      "Test: 17,287 samples √ó 25 features\n",
      "Features: 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING FOR DATA LEAKAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if offer_completed is in features (THIS IS THE PROBLEM!)\n",
    "if 'offer_completed' in X_train.columns:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  DATA LEAKAGE DETECTED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Column 'offer_completed' found in features.\")\n",
    "    print(\"This is IDENTICAL to target, causing perfect 1.0 predictions.\")\n",
    "    print(\"Dropping 'offer_completed' from train and test sets...\")\n",
    "    \n",
    "    X_train.drop('offer_completed', axis=1, inplace=True)\n",
    "    X_test.drop('offer_completed', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"‚úì Dropped. New train shape: {X_train.shape}\")\n",
    "    print(f\"‚úì Features remaining: {len(X_train.columns)}\")\n",
    "else:\n",
    "    print(\"\\n‚úì No 'offer_completed' column found (already removed)\")\n",
    "\n",
    "# Also check for offer_viewed (less severe leak, but worth noting)\n",
    "if 'offer_viewed' in X_train.columns:\n",
    "    print(\"\\n‚ÑπÔ∏è  INFO: 'offer_viewed' feature present\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This feature is a potential data leak.\")\n",
    "    print(\"It's available before completion in real-time scenarios.\")\n",
    "    print(\"For now, we'll keep it to see full model performance.\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nRECOMMENDATION:\")\n",
    "    print(\"For true real-time prediction models, consider:\")\n",
    "    print(\"  1. Train models WITHOUT 'offer_viewed'\")\n",
    "    print(\"  2. For 'post-notification' prediction, keep 'offer_viewed'\")\n",
    "    print(\"\\nFor now, we'll keep it to see full model performance.\")\n",
    "else:\n",
    "    print(\"‚úì No 'offer_viewed' feature found\")\n",
    "\n",
    "# Final verification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA SHAPE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "print(f\"Features: {len(X_train.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb12fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if offer_completed is in features and remove it\n",
    "if 'offer_completed' in X_train.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  DATA LEAKAGE DETECTED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Column 'offer_completed' found in features.\")\n",
    "    print(\"This is IDENTICAL to target, causing perfect predictions.\")\n",
    "    print(\"\\nDropping 'offer_completed' and 'offer_viewed' from train and test sets...\\n\")\n",
    "\n",
    "    X_train = X_train.drop(columns=['offer_completed', 'offer_viewed'], axis=1, inplace=True)\n",
    "    X_test = X_test.drop(columns=['offer_completed', 'offer_viewed'], axis=1, inplace=True)\n",
    "\n",
    "    # Update feature names list\n",
    "    global feature_names\n",
    "    feature_names = [f for f in feature_names if f not in ['offer_completed', 'offer_viewed']]\n",
    "    \n",
    "    print(f\"‚úì Dropped. New shape: {X_train.shape}\")\n",
    "    print(f\"‚úì Features remaining: {len(feature_names)}\")\n",
    "else:\n",
    "    print(\"‚úì No data leakage columns detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_function",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, returning metrics and predictions.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred),\n",
    "        'f1': f1_score(y_test, y_test_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_test_proba)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "    metrics['cv_f1_mean'] = cv_scores.mean()\n",
    "    metrics['cv_f1_std'] = cv_scores.std()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n‚úì Model trained successfully\")\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy:  {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"\\n5-Fold CV F1-Score: {metrics['cv_f1_mean']:.4f} (¬±{metrics['cv_f1_std']:.4f})\")\n",
    "    \n",
    "    # Check for overfitting/underfitting (more nuanced check)\n",
    "    train_test_diff = metrics['train_accuracy'] - metrics['test_accuracy']\n",
    "    cv_test_diff = abs(metrics['cv_f1_mean'] - metrics['f1'])\n",
    "    \n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"  Train acc: {metrics['train_accuracy']:.4f}\")\n",
    "    print(f\"  Test acc: {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"  Difference: {train_test_diff:+.4f}\")\n",
    "    print(f\"  CV F1: {metrics['cv_f1_mean']:.4f} vs Test F1: {metrics['f1']:.4f} (diff: {cv_test_diff:+.4f})\")\n",
    "    \n",
    "    # Overfitting detection criteria\n",
    "    if train_test_diff > 0.15:\n",
    "        print(f\"\\n‚ö†Ô∏è  OVERFITTING DETECTED: Train acc exceeds test by {train_test_diff:.3f}\")\n",
    "    elif train_test_diff < -0.10:\n",
    "        print(f\"\\n‚ö†Ô∏è  UNDERFITTING: Test acc exceeds train by {abs(train_test_diff):.3f}\")\n",
    "    elif cv_test_diff > 0.10:\n",
    "        print(f\"\\n‚ö†Ô∏è  OVERFITTING WARNING: CV F1 exceeds Test F1 by {cv_test_diff:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì No significant overfitting/underfitting\")\n",
    "    \n",
    "    return model, metrics, y_test_pred, y_test_proba\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"Plot confusion matrix for a model.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Not Completed', 'Completed'],\n",
    "                yticklabels=['Not Completed', 'Completed'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba, model_name):\n",
    "    \"\"\"Plot ROC curve for a model.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc_score = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluation functions defined! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline_models",
   "metadata": {},
   "source": [
    "## Baseline Models\n",
    "\n",
    "Start with simple, interpretable models to establish performance baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "lr_model, lr_metrics, lr_pred, lr_proba = evaluate_model(\n",
    "    lr_model, X_train, X_test, y_train, y_test, \"Logistic Regression\"\n",
    ")\n",
    "results.append(lr_metrics)\n",
    "predictions['Logistic Regression'] = lr_pred\n",
    "probabilities['Logistic Regression'] = lr_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_dt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "dt_model, dt_metrics, dt_pred, dt_proba = evaluate_model(\n",
    "    dt_model, X_train, X_test, y_train, y_test, \"Decision Tree\"\n",
    ")\n",
    "results.append(dt_metrics)\n",
    "predictions['Decision Tree'] = dt_pred\n",
    "probabilities['Decision Tree'] = dt_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble_models",
   "metadata": {},
   "source": [
    "## Ensemble Models\n",
    "\n",
    "Train more powerful ensemble models to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "rf_model, rf_metrics, rf_pred, rf_proba = evaluate_model(\n",
    "    rf_model, X_train, X_test, y_train, y_test, \"Random Forest\"\n",
    ")\n",
    "results.append(rf_metrics)\n",
    "predictions['Random Forest'] = rf_pred\n",
    "probabilities['Random Forest'] = rf_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"XGBoost available! ‚úì\")\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    xgb_model, xgb_metrics, xgb_pred, xgb_proba = evaluate_model(\n",
    "        xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\"\n",
    "    )\n",
    "    results.append(xgb_metrics)\n",
    "    predictions['XGBoost'] = xgb_pred\n",
    "    probabilities['XGBoost'] = xgb_proba\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  XGBoost not installed. Installing...\")\n",
    "    print(\"Run: pip install xgboost\")\n",
    "    xgb_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_comparison",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('model')[\n",
    "    ['test_accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'cv_f1_mean', 'cv_f1_std']\n",
    "]\n",
    "results_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC-ROC', 'CV F1 (mean)', 'CV F1 (std)']\n",
    "results_df = results_df.sort_values('F1', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*70)\n",
    "best_model = results_df.index[0]\n",
    "best_f1 = results_df.loc[best_model, 'F1']\n",
    "print(f\"\\nüèÜ  Best Model: {best_model}\")\n",
    "print(f\"   F1-Score: {best_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {results_df.loc[best_model, 'AUC-ROC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy & F1\n",
    "ax1 = axes[0, 0]\n",
    "results_df[['Accuracy', 'F1']].plot(kind='bar', ax=ax1, rot=45)\n",
    "ax1.set_title('Accuracy & F1-Score')\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "ax2 = axes[0, 1]\n",
    "results_df[['Precision', 'Recall']].plot(kind='bar', ax=ax2, rot=45)\n",
    "ax2.set_title('Precision & Recall')\n",
    "ax2.set_ylim(0.5, 1.0)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# AUC-ROC\n",
    "ax3 = axes[1, 0]\n",
    "results_df[['AUC-ROC']].sort_values('AUC-ROC').plot(kind='barh', ax=ax3, color='green')\n",
    "ax3.set_title('AUC-ROC (Higher is Better)')\n",
    "ax3.set_xlim(0.7, 1.0)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# CV Stability\n",
    "ax4 = axes[1, 1]\n",
    "results_df['CV F1 (std)'].plot(kind='bar', ax=ax4, color='orange', rot=45)\n",
    "ax4.set_title('CV Stability (Lower Std is Better)')\n",
    "ax4.set_ylim(0, 0.02)\n",
    "ax4.axhline(y=0.01, color='red', linestyle='--', label='0.01 threshold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_model_analysis",
   "metadata": {},
   "source": [
    "## Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_model_name = results_df.index[0]\n",
    "plot_confusion_matrix(y_test, predictions[best_model_name], best_model_name)\n",
    "\n",
    "# Print classification report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CLASSIFICATION REPORT: {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(classification_report(y_test, predictions[best_model_name], \n",
    "                          target_names=['Not Completed', 'Completed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_roc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for best model\n",
    "plot_roc_curve(y_test, probabilities[best_model_name], best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest (tree-based model)\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    importances = rf_model.feature_importances_\n",
    "    # Use actual column names from X_train (in case features were removed)\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'feature': X_train.columns.tolist(),\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 20 FEATURE IMPORTANCE (Random Forest)\")\n",
    "    print(\"=\"*60)\n",
    "    print(feature_imp_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15 = feature_imp_df.head(15)\n",
    "    plt.barh(top_15['feature'], top_15['importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Most Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Random Forest model not available for feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr_coefficients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression coefficients (for interpretability)\n",
    "if hasattr(lr_model, 'coef_'):\n",
    "    # Use actual column names from X_train (in case features were removed)\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns.tolist(),\n",
    "        'coefficient': lr_model.coef_[0]\n",
    "    })\n",
    "    coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "    coef_df = coef_df.sort_values('abs_coef', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 15 LOGISTIC REGRESSION COEFFICIENTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"(Positive coef = increases completion probability)\")\n",
    "    print(\"(Negative coef = decreases completion probability)\\n\")\n",
    "    print(coef_df.head(15)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter_tuning",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Tune the best model to improve performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf_tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING: RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [20, 50, 100],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "print(f\"\\nParameter grid: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])} combinations\")\n",
    "\n",
    "# Grid search\n",
    "rf_tuned = RandomForestClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_tuned,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV... (this may take several minutes)\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TUNING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "rf_tuned = grid_search.best_estimator_\n",
    "rf_tuned_pred = rf_tuned.predict(X_test)\n",
    "rf_tuned_proba = rf_tuned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tuned_f1 = f1_score(y_test, rf_tuned_pred)\n",
    "tuned_auc = roc_auc_score(y_test, rf_tuned_proba)\n",
    "\n",
    "print(f\"\\nTuned Model Test Performance:\")\n",
    "print(f\"  F1-Score: {tuned_f1:.4f} (baseline: {results_df.loc['Random Forest', 'F1']:.4f})\")\n",
    "print(f\"  AUC-ROC:   {tuned_auc:.4f} (baseline: {results_df.loc['Random Forest', 'AUC-ROC']:.4f})\")\n",
    "\n",
    "improvement = (tuned_f1 - results_df.loc['Random Forest', 'F1']) / results_df.loc['Random Forest', 'F1'] * 100\n",
    "print(f\"\\nImprovement: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_models",
   "metadata": {},
   "source": [
    "## Save Models\n",
    "\n",
    "Save all trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "models_dir = '../Cafe_Rewards_Offers/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "models_to_save = {\n",
    "    'logistic_regression.pkl': lr_model,\n",
    "    'decision_tree.pkl': dt_model,\n",
    "    'random_forest.pkl': rf_model,\n",
    "    'random_forest_tuned.pkl': rf_tuned\n",
    "}\n",
    "\n",
    "if xgb_model is not None:\n",
    "    models_to_save['xgboost.pkl'] = xgb_model\n",
    "\n",
    "for filename, model in models_to_save.items():\n",
    "    joblib.dump(model, f'{models_dir}/{filename}')\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f'{models_dir}/model_comparison.csv')\n",
    "print(f\"\\n‚úì Saved: model_comparison.csv\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL MODELS SAVED\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modeling_summary",
   "metadata": {},
   "source": [
    "## Modeling Summary\n",
    "\n",
    "**Completed Steps:**\n",
    "1. ‚úì Loaded processed data\n",
    "2. ‚úì Checked for data leakage\n",
    "3. ‚úì Trained 4 baseline/ensemble models\n",
    "4. ‚úì Evaluated using multiple metrics (Accuracy, Precision, Recall, F1, AUC-ROC)\n",
    "5. ‚úì Compared model performance\n",
    "6. ‚úì Identified best performing model\n",
    "7. ‚úì Analyzed feature importance\n",
    "8. ‚úì Performed hyperparameter tuning\n",
    "9. ‚úì Saved all models\n",
    "\n",
    "**Next Steps:**\n",
    "1. PCA for dimensionality reduction\n",
    "2. SHAP analysis for model explainability\n",
    "3. Bias & fairness analysis\n",
    "4. Create presentations (technical & business)\n",
    "5. Upload to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "next_steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODELING COMPLETE! ‚úì\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Best F1-Score: {results_df.loc[best_model, 'F1']:.4f}\")\n",
    "print(f\"\\nAll models saved to: {models_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3de5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_beansage (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
