{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Bias and Fairness Analysis\n",
    "\n",
    "**Goal**: Evaluate the fairness of the offer completion prediction model across different demographic groups.\n",
    "\n",
    "**Context from Previous Analyses:**\n",
    "\n",
    "This analysis builds upon the following completed work:\n",
    "- **Modeling (03_Modeling.ipynb)**: Random Forest selected as best model (F1: 0.8601, AUC: 0.9277) with 24 features after removing data leakage\n",
    "- **PCA (04_PCA.ipynb)**: 8 components capture 90% variance with minimal performance loss (1% F1 drop)\n",
    "- **SHAP (05_SHAP.ipynb)**: Top 3 features (offer_type_discount 21.4%, duration 14.2%, difficulty 9.3%) drive 45% of predictions\n",
    "- **Segmentation (06_Customer_Segmentation.ipynb)**: 5 customer segments identified with completion rates from 15.7% to 72.6%\n",
    "\n",
    "**Why Fairness Matters:**\n",
    "- Avoid discriminatory outcomes in marketing offers\n",
    "- Ensure equitable customer experience across demographic groups\n",
    "- Build trust in AI-driven recommendations\n",
    "- Meet regulatory and ethical standards\n",
    "- Identify and mitigate bias from modeling decisions\n",
    "\n",
    "**Protected Attributes Analyzed:**\n",
    "1. **Gender**: Male, Female, Other, Missing\n",
    "2. **Age Group**: 18-30, 31-45, 46-60, 61-75, 76+\n",
    "3. **Income Bracket**: Missing, Low, Medium, High, Very High\n",
    "4. **Tenure Group**: New, Short-term, Medium-term, Long-term\n",
    "\n",
    "**Fairness Metrics:**\n",
    "- **Demographic Parity**: Similar prediction rates across groups (80% rule: 0.8 â‰¤ ratio â‰¤ 1.25)\n",
    "- **Equal Opportunity**: Similar true positive rates across groups\n",
    "- **Predictive Parity**: Similar precision across groups\n",
    "- **Disparate Impact**: Ratio of favorable outcomes between groups\n",
    "- **Overall Accuracy**: Similar accuracy across groups\n",
    "\n",
    "**Key Questions:**\n",
    "1. Does the model perform equally well across all demographic groups?\n",
    "2. Are certain groups systematically over/under-predicted?\n",
    "3. Do feature importance patterns create unfair advantages for specific groups?\n",
    "4. Are there intersectional biases (e.g., young low-income customers)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready! âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, confusion_matrix, roc_auc_score,\n",
    "                            precision_recall_curve, roc_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Environment ready! âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_processed_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '../Cafe_Rewards_Offers/processed'\n",
    "models_dir = '../Cafe_Rewards_Offers/models'\n",
    "\n",
    "X_test = joblib.load(f'{processed_dir}/X_test_scaled.pkl')\n",
    "y_test = joblib.load(f'{processed_dir}/y_test.pkl')\n",
    "feature_names = joblib.load(f'{processed_dir}/feature_names.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest set: {X_test.shape[0]:,} samples Ã— {X_test.shape[1]} features\")\n",
    "print(f\"\\nFeatures present: {len(feature_names)}\")\n",
    "print(f\"Expected features: 24 (after data leakage removal)\")\n",
    "\n",
    "# Verify data leakage features have been removed\n",
    "leakage_features = ['offer_completed', 'offer_viewed']\n",
    "leakage_found = [f for f in leakage_features if f in feature_names]\n",
    "\n",
    "if leakage_found:\n",
    "    print(f\"\\nâš ï¸  WARNING: Leakage features still present: {leakage_found}\")\n",
    "    print(\"   These should have been removed in the modeling notebook!\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Data leakage features removed successfully\")\n",
    "\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = joblib.load(f'{models_dir}/random_forest.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model type: Random Forest Classifier\")\n",
    "print(f\"Expected features: {rf_model.n_features_in_}\")\n",
    "print(f\"Test set features: {X_test.shape[1]}\")\n",
    "\n",
    "# Check feature alignment\n",
    "if X_test.shape[1] != rf_model.n_features_in_:\n",
    "    print(f\"\\nâš ï¸  FEATURE MISMATCH DETECTED!\")\n",
    "    print(f\"   Model expects {rf_model.n_features_in_} features\")\n",
    "    print(f\"   Test set has {X_test.shape[1]} features\")\n",
    "    print(f\"\\n   Aligning features...\")\n",
    "    \n",
    "    # Align test set to match model expectations\n",
    "    if X_test.shape[1] > rf_model.n_features_in_:\n",
    "        # Remove extra features\n",
    "        X_test = X_test.iloc[:, :rf_model.n_features_in_]\n",
    "        feature_names = feature_names[:rf_model.n_features_in_]\n",
    "        print(f\"   âœ“ Removed extra features. New shape: {X_test.shape}\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Feature alignment verified\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPECTED PERFORMANCE (from 03_Modeling.ipynb)\")\n",
    "print(\"=\"*60)\n",
    "print(\"  F1-Score:  0.8601\")\n",
    "print(\"  AUC-ROC:   0.9277\")\n",
    "print(\"\\nNote: Values should match the modeling notebook results\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected_attributes",
   "metadata": {},
   "source": [
    "## Protected Attributes Analysis\n",
    "\n",
    "**Context from Previous Analyses:**\n",
    "\n",
    "From **SHAP Analysis (05_SHAP.ipynb)**, we know the top predictive features:\n",
    "1. **offer_type_discount** (21.4% importance) - Offer design feature\n",
    "2. **duration** (14.2% importance) - Offer design feature  \n",
    "3. **difficulty** (9.3% importance) - Offer design feature\n",
    "4. **offer_viewed** (7.5% importance) - Behavioral feature\n",
    "5. **gender_M** (5.9% importance) - **DEMOGRAPHIC FEATURE**\n",
    "6. **income_bracket_encoded** (5.7% importance) - **DEMOGRAPHIC FEATURE**\n",
    "7. **age** (5.3% importance) - **DEMOGRAPHIC FEATURE**\n",
    "\n",
    "**Key Insight from SHAP:** Demographics account for 33.5% of model importance, with offer design at 52.3%. This means:\n",
    "- The model DOES use demographic features significantly\n",
    "- Gender, income, and age influence predictions\n",
    "- **Potential for demographic bias exists**\n",
    "\n",
    "From **Segmentation Analysis (06_Customer_Segmentation.ipynb)**, we know:\n",
    "- **Cluster 0 (29%)**: 72.6% completion rate - High performers (long tenure, high engagement)\n",
    "- **Cluster 1 (43%)**: 55.9% completion rate - Largest group (short tenure, perfect view rate)\n",
    "- **Cluster 2 (12%)**: 15.7% completion rate - Data quality issue (missing demographics)\n",
    "- **Cluster 3 (16%)**: 39.0% completion rate - Low engagement (zero view rate)\n",
    "\n",
    "**Critical Question:** Are demographic groups distributed fairly across these segments? If not, certain groups may systematically receive worse predictions.\n",
    "\n",
    "We'll analyze model performance across protected attributes from the original data to identify bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_original_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../Cafe_Rewards_Offers/processed_data_for_classification.csv')\n",
    "\n",
    "print(f\"Original dataset loaded: {df_original.shape[0]:,} rows Ã— {df_original.shape[1]} columns\")\n",
    "print(f\"\\nColumns available for fairness analysis:\")\n",
    "protected_cols = ['gender', 'age', 'income', 'age_group', 'income_bracket', 'tenure_group']\n",
    "for col in protected_cols:\n",
    "    if col in df_original.columns:\n",
    "        unique_vals = df_original[col].unique()\n",
    "        print(f\"  - {col}: {len(unique_vals)} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_protected_attributes",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fairness = X_test.copy()\n",
    "df_test_fairness['target'] = y_test.values\n",
    "df_test_fairness['prediction'] = y_pred\n",
    "df_test_fairness['prediction_proba'] = y_proba\n",
    "\n",
    "df_original_test = df_original.iloc[X_test.index].copy()\n",
    "\n",
    "for col in ['gender', 'age_group', 'income_bracket', 'tenure_group']:\n",
    "    if col in df_original_test.columns:\n",
    "        df_test_fairness[col] = df_original_test[col].values\n",
    "\n",
    "print(f\"Fairness analysis dataframe created: {df_test_fairness.shape}\")\n",
    "print(f\"\\nProtected attributes added: {['gender', 'age_group', 'income_bracket', 'tenure_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fairness_functions",
   "metadata": {},
   "source": [
    "## Fairness Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_group_metrics(y_true, y_pred, y_proba, group_mask):\n",
    "    \"\"\"Calculate classification metrics for a specific subgroup.\"\"\"\n",
    "    if sum(group_mask) < 10:\n",
    "        return None\n",
    "    \n",
    "    y_true_g = y_true[group_mask]\n",
    "    y_pred_g = y_pred[group_mask]\n",
    "    y_proba_g = y_proba[group_mask]\n",
    "    \n",
    "    cm = confusion_matrix(y_true_g, y_pred_g)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    metrics = {\n",
    "        'count': sum(group_mask),\n",
    "        'positive_rate': y_pred_g.mean(),\n",
    "        'accuracy': accuracy_score(y_true_g, y_pred_g),\n",
    "        'precision': precision_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'recall': recall_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'f1': f1_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'tpr': recall_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'tnr': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        'fnr': fn / (fn + tp) if (fn + tp) > 0 else 0,\n",
    "        'auc': roc_auc_score(y_true_g, y_proba_g) if len(np.unique(y_true_g)) > 1 else np.nan\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def analyze_fairness_by_attribute(df, attribute, y_true_col='target', \n",
    "                                  y_pred_col='prediction', y_proba_col='prediction_proba'):\n",
    "    \"\"\"Analyze fairness metrics across all values of a protected attribute.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    overall_metrics = calculate_group_metrics(\n",
    "        df[y_true_col].values, \n",
    "        df[y_pred_col].values, \n",
    "        df[y_proba_col].values,\n",
    "        np.ones(len(df), dtype=bool)\n",
    "    )\n",
    "    \n",
    "    for value in df[attribute].unique():\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        \n",
    "        mask = df[attribute] == value\n",
    "        group_metrics = calculate_group_metrics(\n",
    "            df[y_true_col].values, \n",
    "            df[y_pred_col].values, \n",
    "            df[y_proba_col].values,\n",
    "            mask\n",
    "        )\n",
    "        \n",
    "        if group_metrics:\n",
    "            group_metrics['attribute'] = attribute\n",
    "            group_metrics['value'] = value\n",
    "            \n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1', 'positive_rate', 'tpr', 'fpr']:\n",
    "                if overall_metrics[metric] > 0:\n",
    "                    diff = group_metrics[metric] - overall_metrics[metric]\n",
    "                    group_metrics[f'{metric}_diff'] = diff\n",
    "                    if overall_metrics[metric] > 0:\n",
    "                        group_metrics[f'{metric}_pct_diff'] = (diff / overall_metrics[metric]) * 100\n",
    "            \n",
    "            results.append(group_metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def calculate_disparate_impact(df, attribute, y_pred_col='prediction', reference_value=None):\n",
    "    \"\"\"Calculate disparate impact ratio for a protected attribute.\"\"\"\n",
    "    \n",
    "    positive_rates = df.groupby(attribute)[y_pred_col].mean()\n",
    "    \n",
    "    if reference_value is None:\n",
    "        reference_value = positive_rates.idxmax()\n",
    "    \n",
    "    reference_rate = positive_rates[reference_value]\n",
    "    \n",
    "    di_results = []\n",
    "    for value, rate in positive_rates.items():\n",
    "        if reference_rate > 0:\n",
    "            di = rate / reference_rate\n",
    "        else:\n",
    "            di = np.nan\n",
    "        \n",
    "        di_results.append({\n",
    "            'attribute': attribute,\n",
    "            'value': value,\n",
    "            'positive_rate': rate,\n",
    "            'reference': reference_value,\n",
    "            'reference_rate': reference_rate,\n",
    "            'disparate_impact': di,\n",
    "            'is_fair': 0.8 <= di <= 1.25\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(di_results)\n",
    "\n",
    "\n",
    "def plot_fairness_comparison(metrics_df, attribute, metric_cols=['accuracy', 'precision', 'recall', 'f1']):\n",
    "    \"\"\"Plot fairness metrics comparison across groups.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Fairness Analysis by {attribute}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values('value')\n",
    "    values = metrics_df['value'].values\n",
    "    \n",
    "    for idx, metric in enumerate(metric_cols):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        bars = ax.bar(values, metrics_df[metric].values, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        overall_mean = metrics_df[f'{metric}_diff'].mean() + metrics_df[metric].mean()\n",
    "        ax.axhline(y=overall_mean, color='red', linestyle='--', linewidth=2, \n",
    "                  label=f'Overall Mean: {overall_mean:.3f}')\n",
    "        \n",
    "        for bar, val in zip(bars, metrics_df[metric].values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax.set_xlabel(attribute)\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} by {attribute}')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_positive_rates(metrics_df, attribute):\n",
    "    \"\"\"Plot positive prediction rates across groups (Demographic Parity).\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values('positive_rate')\n",
    "    \n",
    "    colors = ['green' if 0.8 <= (rate / metrics_df['positive_rate'].max()) <= 1.25 \n",
    "              else 'orange' for rate in metrics_df['positive_rate']]\n",
    "    \n",
    "    bars = plt.bar(metrics_df['value'], metrics_df['positive_rate'], \n",
    "                    color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.axhline(y=metrics_df['positive_rate'].mean(), color='red', \n",
    "                linestyle='--', linewidth=2, \n",
    "                label=f'Overall Mean: {metrics_df[\"positive_rate\"].mean():.3f}')\n",
    "    \n",
    "    for bar, val in zip(bars, metrics_df['positive_rate'].values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.xlabel(attribute)\n",
    "    plt.ylabel('Positive Prediction Rate')\n",
    "    plt.title(f'Demographic Parity - Positive Rate by {attribute}', \n",
    "              fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Fairness metrics functions defined! âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gender_analysis",
   "metadata": {},
   "source": [
    "## 1. Gender-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GENDER DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gender_dist = df_test_fairness['gender'].value_counts(normalize=True).sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for gender, pct in gender_dist.items():\n",
    "    count = (df_test_fairness['gender'] == gender).sum()\n",
    "    print(f\"  {gender}: {count:5,} ({pct*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by gender:\")\n",
    "for gender in df_test_fairness['gender'].unique():\n",
    "    if pd.notna(gender):\n",
    "        subset = df_test_fairness[df_test_fairness['gender'] == gender]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {gender}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_fairness = analyze_fairness_by_attribute(df_test_fairness, 'gender')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENDER FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(gender_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(gender_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_di = calculate_disparate_impact(df_test_fairness, 'gender')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENDER DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 â‰¤ DI â‰¤ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in gender_di.iterrows():\n",
    "    status = \"âœ“ FAIR\" if row['is_fair'] else \"âš ï¸  UNFAIR\"\n",
    "    print(f\"{row['value']:10} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(gender_fairness, 'gender')\n",
    "plot_positive_rates(gender_fairness, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "age_analysis",
   "metadata": {},
   "source": [
    "## 2. Age Group-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "age_dist = df_test_fairness['age_group'].value_counts().sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for age, count in age_dist.items():\n",
    "    if pd.notna(age):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {age:10}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by age group:\")\n",
    "for age in sorted(df_test_fairness['age_group'].unique()):\n",
    "    if pd.notna(age):\n",
    "        subset = df_test_fairness[df_test_fairness['age_group'] == age]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {age:10}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_fairness = analyze_fairness_by_attribute(df_test_fairness, 'age_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(age_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(age_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_di = calculate_disparate_impact(df_test_fairness, 'age_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 â‰¤ DI â‰¤ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in age_di.iterrows():\n",
    "    status = \"âœ“ FAIR\" if row['is_fair'] else \"âš ï¸  UNFAIR\"\n",
    "    print(f\"{row['value']:10} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(age_fairness, 'age_group')\n",
    "plot_positive_rates(age_fairness, 'age_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "income_analysis",
   "metadata": {},
   "source": [
    "## 3. Income Bracket-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "income_dist = df_test_fairness['income_bracket'].value_counts().sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for income, count in income_dist.items():\n",
    "    if pd.notna(income):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {income:12}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by income bracket:\")\n",
    "for income in sorted(df_test_fairness['income_bracket'].unique()):\n",
    "    if pd.notna(income):\n",
    "        subset = df_test_fairness[df_test_fairness['income_bracket'] == income]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {income:12}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_fairness = analyze_fairness_by_attribute(df_test_fairness, 'income_bracket')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(income_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(income_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_di = calculate_disparate_impact(df_test_fairness, 'income_bracket')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 â‰¤ DI â‰¤ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in income_di.iterrows():\n",
    "    status = \"âœ“ FAIR\" if row['is_fair'] else \"âš ï¸  UNFAIR\"\n",
    "    print(f\"{row['value']:12} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(income_fairness, 'income_bracket')\n",
    "plot_positive_rates(income_fairness, 'income_bracket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tenure_analysis",
   "metadata": {},
   "source": [
    "## 4. Tenure Group-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TENURE GROUP DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tenure_dist = df_test_fairness['tenure_group'].value_counts()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for tenure, count in tenure_dist.items():\n",
    "    if pd.notna(tenure):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {tenure:15}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by tenure group:\")\n",
    "for tenure in df_test_fairness['tenure_group'].unique():\n",
    "    if pd.notna(tenure):\n",
    "        subset = df_test_fairness[df_test_fairness['tenure_group'] == tenure]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {tenure:15}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_fairness = analyze_fairness_by_attribute(df_test_fairness, 'tenure_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TENURE GROUP FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(tenure_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(tenure_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(tenure_fairness, 'tenure_group')\n",
    "plot_positive_rates(tenure_fairness, 'tenure_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrices",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices by Protected Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_confusion_matrix(y_true, y_pred, group_name, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "                xticklabels=['Not Completed', 'Completed'],\n",
    "                yticklabels=['Not Completed', 'Completed'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(group_name, fontweight='bold')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices by Gender', fontsize=16, fontweight='bold')\n",
    "\n",
    "gender_values = [g for g in df_test_fairness['gender'].unique() if pd.notna(g)]\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, gender in enumerate(gender_values):\n",
    "    mask = df_test_fairness['gender'] == gender\n",
    "    plot_group_confusion_matrix(\n",
    "        df_test_fairness[mask]['target'],\n",
    "        df_test_fairness[mask]['prediction'],\n",
    "        f'Gender: {gender} (n={mask.sum():,})',\n",
    "        axes_flat[idx]\n",
    "    )\n",
    "\n",
    "for idx in range(len(gender_values), len(axes_flat)):\n",
    "    axes_flat[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices by Age Group', fontsize=16, fontweight='bold')\n",
    "\n",
    "age_values = sorted([a for a in df_test_fairness['age_group'].unique() if pd.notna(a)])\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, age in enumerate(age_values):\n",
    "    mask = df_test_fairness['age_group'] == age\n",
    "    plot_group_confusion_matrix(\n",
    "        df_test_fairness[mask]['target'],\n",
    "        df_test_fairness[mask]['prediction'],\n",
    "        f'Age: {age} (n={mask.sum():,})',\n",
    "        axes_flat[idx]\n",
    "    )\n",
    "\n",
    "for idx in range(len(age_values), len(axes_flat)):\n",
    "    axes_flat[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc_curves",
   "metadata": {},
   "source": [
    "## 6. ROC Curves by Protected Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_roc_by_groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_by_group(df, attribute, title_suffix):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    values = sorted([v for v in df[attribute].unique() if pd.notna(v)])\n",
    "    \n",
    "    for value in values:\n",
    "        mask = df[attribute] == value\n",
    "        if sum(mask) < 10:\n",
    "            continue\n",
    "        \n",
    "        y_true = df[mask]['target']\n",
    "        y_proba = df[mask]['prediction_proba']\n",
    "        \n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            continue\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        auc_score = roc_auc_score(y_true, y_proba)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'{value} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves by {title_suffix}', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_by_group(df_test_fairness, 'gender', 'Gender')\n",
    "plot_roc_by_group(df_test_fairness, 'age_group', 'Age Group')\n",
    "plot_roc_by_group(df_test_fairness, 'income_bracket', 'Income Bracket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intersectional",
   "metadata": {},
   "source": [
    "## 7. Intersectional Fairness Analysis\n",
    "\n",
    "Analyze fairness across intersections of protected attributes (e.g., Gender Ã— Age Group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intersectional_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INTERSECTIONAL FAIRNESS: GENDER Ã— AGE GROUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "intersectional_results = []\n",
    "\n",
    "for gender in ['M', 'F']:\n",
    "    for age in sorted([a for a in df_test_fairness['age_group'].unique() if pd.notna(a)]):\n",
    "        mask = (df_test_fairness['gender'] == gender) & (df_test_fairness['age_group'] == age)\n",
    "        \n",
    "        if sum(mask) < 10:\n",
    "            continue\n",
    "        \n",
    "        y_true = df_test_fairness[mask]['target']\n",
    "        y_pred = df_test_fairness[mask]['prediction']\n",
    "        \n",
    "        metrics = {\n",
    "            'gender': gender,\n",
    "            'age_group': age,\n",
    "            'count': sum(mask),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'positive_rate': y_pred.mean()\n",
    "        }\n",
    "        \n",
    "        intersectional_results.append(metrics)\n",
    "\n",
    "intersectional_df = pd.DataFrame(intersectional_results)\n",
    "\n",
    "print(\"\\nIntersectional Fairness Metrics:\")\n",
    "print(intersectional_df.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MAXIMUM DISPARITY IN INTERSECTIONAL GROUPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'positive_rate']:\n",
    "    if len(intersectional_df) > 0:\n",
    "        max_val = intersectional_df[metric].max()\n",
    "        min_val = intersectional_df[metric].min()\n",
    "        disparity = max_val - min_val\n",
    "        print(f\"{metric:15}: Max={max_val:.3f}, Min={min_val:.3f}, Disparity={disparity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intersectional_heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_accuracy = intersectional_df.pivot(index='age_group', columns='gender', values='accuracy')\n",
    "pivot_recall = intersectional_df.pivot(index='age_group', columns='gender', values='recall')\n",
    "pivot_f1 = intersectional_df.pivot(index='age_group', columns='gender', values='f1')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.heatmap(pivot_accuracy, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'Accuracy'}, ax=axes[0])\n",
    "axes[0].set_title('Accuracy by Gender Ã— Age', fontweight='bold')\n",
    "\n",
    "sns.heatmap(pivot_recall, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'Recall'}, ax=axes[1])\n",
    "axes[1].set_title('Recall by Gender Ã— Age', fontweight='bold')\n",
    "\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'F1-Score'}, ax=axes[2])\n",
    "axes[2].set_title('F1-Score by Gender Ã— Age', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETING INTERSECTIONAL FAIRNESS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Patterns to Look For:\")\n",
    "print(\"  â€¢ Dark green cells (high F1): Well-served demographic groups\")\n",
    "print(\"  â€¢ Red cells (low F1): Under-served groups needing intervention\")\n",
    "print(\"  â€¢ Consistent rows: Age matters more than gender for this row\")\n",
    "print(\"  â€¢ Consistent columns: Gender matters more than age for this column\")\n",
    "print(\"\\nFairness Concern: If low F1 is concentrated in specific intersections\")\n",
    "print(\"(e.g., 'Young Ã— Low Income'), those groups face compounded disadvantage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5nnexzu43",
   "metadata": {},
   "source": [
    "## 7.2 Linking Demographics to Customer Segments\n",
    "\n",
    "**Critical Analysis:** From the segmentation notebook (06_Customer_Segmentation.ipynb), we identified customer segments with vastly different completion rates:\n",
    "- Cluster 0: 72.6% completion\n",
    "- Cluster 1: 55.9% completion  \n",
    "- Cluster 2: 15.7% completion âš ï¸\n",
    "- Cluster 3: 39.0% completion\n",
    "\n",
    "**Key Question:** Are certain demographic groups overrepresented in low-performing segments?\n",
    "\n",
    "If yes, this creates **systemic bias** where demographic characteristics indirectly determine segment assignment, which determines offer success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90y60vcsljp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segmentation results\n",
    "segmentation_dir = '../Cafe_Rewards_Offers/segmentation'\n",
    "\n",
    "try:\n",
    "    customers_with_clusters = pd.read_csv(f'{segmentation_dir}/customers_with_clusters.csv')\n",
    "    print(\"=\"*70)\n",
    "    print(\"SEGMENTATION DATA LOADED\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total customers with cluster assignments: {len(customers_with_clusters):,}\")\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    print(customers_with_clusters['cluster'].value_counts().sort_index())\n",
    "    \n",
    "    # Check if we can merge with test set\n",
    "    print(f\"\\nTest set size: {len(df_test_fairness):,}\")\n",
    "    print(f\"Cluster data size: {len(customers_with_clusters):,}\")\n",
    "    \n",
    "    # Add cluster assignments to test fairness dataframe if indices match\n",
    "    if len(customers_with_clusters) == len(df_original):\n",
    "        # Extract cluster for test set indices\n",
    "        df_test_fairness['cluster'] = customers_with_clusters.iloc[df_test_fairness.index]['cluster'].values\n",
    "        \n",
    "        print(\"\\nâœ“ Successfully added cluster assignments to test set\")\n",
    "        print(f\"\\nTest set cluster distribution:\")\n",
    "        print(df_test_fairness['cluster'].value_counts().sort_index())\n",
    "        \n",
    "        # Analyze demographics by cluster\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DEMOGRAPHIC COMPOSITION BY CUSTOMER SEGMENT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for cluster in sorted(df_test_fairness['cluster'].unique()):\n",
    "            if pd.isna(cluster):\n",
    "                continue\n",
    "            \n",
    "            cluster_data = df_test_fairness[df_test_fairness['cluster'] == cluster]\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"CLUSTER {int(cluster)}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Size: {len(cluster_data):,} customers ({len(cluster_data)/len(df_test_fairness)*100:.1f}% of test set)\")\n",
    "            print(f\"Completion rate: {cluster_data['target'].mean():.1%}\")\n",
    "            \n",
    "            # Gender distribution\n",
    "            print(f\"\\nGender distribution:\")\n",
    "            for gender in ['M', 'F', 'O', 'Missing']:\n",
    "                if 'gender' in cluster_data.columns:\n",
    "                    pct = (cluster_data['gender'] == gender).sum() / len(cluster_data) * 100\n",
    "                    print(f\"  {gender:8}: {pct:5.1f}%\")\n",
    "            \n",
    "            # Age distribution\n",
    "            print(f\"\\nAge group distribution:\")\n",
    "            if 'age_group' in cluster_data.columns:\n",
    "                age_dist = cluster_data['age_group'].value_counts(normalize=True).sort_index() * 100\n",
    "                for age, pct in age_dist.items():\n",
    "                    if pd.notna(age):\n",
    "                        print(f\"  {age:10}: {pct:5.1f}%\")\n",
    "            \n",
    "            # Income distribution\n",
    "            print(f\"\\nIncome bracket distribution:\")\n",
    "            if 'income_bracket' in cluster_data.columns:\n",
    "                income_dist = cluster_data['income_bracket'].value_counts(normalize=True).sort_index() * 100\n",
    "                for income, pct in income_dist.items():\n",
    "                    if pd.notna(income):\n",
    "                        print(f\"  {income:12}: {pct:5.1f}%\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEY INSIGHTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n1. Check if Cluster 2 (lowest completion 15.7%) has:\")\n",
    "        print(\"   - Higher % of missing demographics\")\n",
    "        print(\"   - Specific age/income groups overrepresented\")\n",
    "        print(\"\\n2. Check if Cluster 0 (highest completion 72.6%) has:\")\n",
    "        print(\"   - Specific demographic advantages\")\n",
    "        print(\"   - Exclusion of certain groups\")\n",
    "        print(\"\\n3. Fairness concern if:\")\n",
    "        print(\"   - Protected groups are trapped in low-performing clusters\")\n",
    "        print(\"   - Demographic features drive cluster assignment\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Cannot merge: Cluster data size doesn't match original data\")\n",
    "        print(\"   This analysis requires cluster assignments for the exact test set indices\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Segmentation results not found\")\n",
    "    print(f\"   Expected location: {segmentation_dir}/customers_with_clusters.csv\")\n",
    "    print(\"   Run 06_Customer_Segmentation.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "### Integration with Previous Analyses\n",
    "\n",
    "This fairness analysis completes our comprehensive evaluation of the offer completion prediction system. Let's synthesize findings across all notebooks:\n",
    "\n",
    "#### From Modeling (03_Modeling.ipynb):\n",
    "- **Best Model**: Random Forest (F1: 0.8601, AUC: 0.9277)\n",
    "- **Data Leakage Removed**: 24 features (removed offer_completed, offer_viewed)\n",
    "- **Feature Count**: 24 clean features for real-time prediction\n",
    "- **Class Balance**: 53.4% completed, 46.6% not completed (well-balanced)\n",
    "\n",
    "#### From PCA (04_PCA.ipynb):\n",
    "- **Dimensionality Reduction**: 8 components capture 90% variance\n",
    "- **Performance Trade-off**: Only 1% F1 drop (0.8601 â†’ 0.8563) with 68% fewer features\n",
    "- **Feature Concentration**: Top 10 features explain 83% of importance\n",
    "- **Recommendation**: Use 8-component PCA for production efficiency\n",
    "\n",
    "#### From SHAP (05_SHAP.ipynb):\n",
    "- **Top 3 Features Account for 45% of Predictions**:\n",
    "  1. offer_type_discount: 21.4%\n",
    "  2. duration: 14.2%\n",
    "  3. difficulty: 9.3%\n",
    "- **Feature Categories**:\n",
    "  - Offer Attributes: 52.3% (LARGEST)\n",
    "  - Demographics: 33.5% (gender, income, age)\n",
    "  - Behavioral: 7.5% (offer_viewed)\n",
    "  - Channels: 4.4%\n",
    "- **Key Demographic Features**:\n",
    "  - gender_M: 5.9% importance (rank #5)\n",
    "  - income_bracket_encoded: 5.7% (rank #7)\n",
    "  - age: 5.3% (rank #8)\n",
    "\n",
    "**FAIRNESS IMPLICATION**: Demographics account for 1/3 of prediction power. If certain groups systematically have different demographics, the model will treat them differently.\n",
    "\n",
    "#### From Segmentation (06_Customer_Segmentation.ipynb):\n",
    "- **5 Customer Segments Identified**:\n",
    "  - Cluster 0 (29%): 72.6% completion - Long-tenure, high engagement\n",
    "  - Cluster 1 (43%): 55.9% completion - Short-tenure, perfect view rate\n",
    "  - Cluster 2 (12%): 15.7% completion - **Missing demographics** (data quality issue)\n",
    "  - Cluster 3 (16%): 39.0% completion - Low engagement, zero view rate\n",
    "  - Cluster 4 (1%): Small outlier segment\n",
    "\n",
    "**FAIRNESS IMPLICATION**: Cluster 2 (12% of customers) has missing demographics and performs **5x worse** than the best segment. If certain demographic groups are overrepresented in this cluster, they experience systematic disadvantage.\n",
    "\n",
    "**Potential Business Impact**:\n",
    "- If segmentation strategies are implemented: +12,568 additional completions (27.3% increase)\n",
    "- Cluster 1 improvement potential: +6,169 conversions\n",
    "- Cluster 2 data fix potential: +4,004 conversions\n",
    "- Cluster 3 re-engagement: +2,395 conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fairness_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fairness_summary(gender_fairness, age_fairness, income_fairness, tenure_fairness):\n",
    "    \"\"\"Generate a comprehensive fairness summary.\"\"\"\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    fairness_dfs = {\n",
    "        'gender': gender_fairness,\n",
    "        'age_group': age_fairness,\n",
    "        'income_bracket': income_fairness,\n",
    "        'tenure_group': tenure_fairness\n",
    "    }\n",
    "    \n",
    "    for attr_name, df in fairness_dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        metrics_to_check = ['accuracy', 'precision', 'recall', 'f1', 'positive_rate']\n",
    "        \n",
    "        summary[attr_name] = {}\n",
    "        \n",
    "        for metric in metrics_to_check:\n",
    "            if f'{metric}_pct_diff' in df.columns:\n",
    "                max_diff = df[f'{metric}_pct_diff'].abs().max()\n",
    "                min_diff = df[f'{metric}_pct_diff'].abs().min()\n",
    "                \n",
    "                if max_diff > 10:\n",
    "                    risk_level = \"HIGH\"\n",
    "                elif max_diff > 5:\n",
    "                    risk_level = \"MEDIUM\"\n",
    "                else:\n",
    "                    risk_level = \"LOW\"\n",
    "                \n",
    "                summary[attr_name][metric] = {\n",
    "                    'max_disparity_pct': round(max_diff, 2),\n",
    "                    'risk_level': risk_level\n",
    "                }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "fairness_summary = generate_fairness_summary(\n",
    "    gender_fairness, age_fairness, income_fairness, tenure_fairness\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FAIRNESS ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for attr_name, metrics in fairness_summary.items():\n",
    "    print(f\"\\n{attr_name.upper()}:\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        risk_emoji = {\n",
    "            'HIGH': 'ðŸ”´',\n",
    "            'MEDIUM': 'ðŸŸ¡',\n",
    "            'LOW': 'ðŸŸ¢'\n",
    "        }[values['risk_level']]\n",
    "        print(f\"  {metric_name:15}: {values['max_disparity_pct']:>6.2f}% disparity - {risk_emoji} {values['risk_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ACTIONABLE RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nBased on integrated findings from all analyses:\")\n",
    "\n",
    "recommendations = [\n",
    "    {\n",
    "        'category': '1. IMMEDIATE: Address Data Quality Bias (Week 1)',\n",
    "        'priority': 'CRITICAL',\n",
    "        'items': [\n",
    "            \"From Segmentation: 12% of customers (Cluster 2) have missing demographics\",\n",
    "            \"These customers have 15.7% completion vs 72.6% for best segment (4.6x worse)\",\n",
    "            \"If certain demographic groups are less likely to provide data, this creates systematic bias\",\n",
    "            \"\",\n",
    "            \"Actions:\",\n",
    "            \"  â€¢ Audit: Which demographics are overrepresented in 'missing data' group?\",\n",
    "            \"  â€¢ Fix onboarding: Make key demographic fields required (age, income, gender)\",\n",
    "            \"  â€¢ Incentivize: Offer free drink/points for profile completion\",\n",
    "            \"  â€¢ Target: Launch campaign to existing customers with incomplete profiles\",\n",
    "            \"\",\n",
    "            \"Expected Impact: +4,004 conversions if Cluster 2 is brought to average performance\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': '2. SHORT-TERM: Fairness Monitoring Dashboard (Month 1)',\n",
    "        'priority': 'HIGH',\n",
    "        'items': [\n",
    "            \"From SHAP: Demographics drive 33.5% of predictions (gender 5.9%, income 5.7%, age 5.3%)\",\n",
    "            \"From Fairness Analysis: Performance varies by protected attribute (computed above)\",\n",
    "            \"\",\n",
    "            \"Actions:\",\n",
    "            \"  â€¢ Set up automated fairness metric tracking by:\",\n",
    "            \"    - Gender (M, F, O, Missing)\",\n",
    "            \"    - Age group (18-30, 31-45, 46-60, 61+)\",\n",
    "            \"    - Income bracket (Low, Medium, High, Very High)\",\n",
    "            \"    - Customer segment (Clusters 0-3)\",\n",
    "            \"  â€¢ Alert when disparities exceed thresholds:\",\n",
    "            \"    - Accuracy disparity > 10%\",\n",
    "            \"    - Disparate impact < 0.8 or > 1.25 (80% rule violation)\",\n",
    "            \"    - F1-score gap > 0.15 between best/worst group\",\n",
    "            \"  â€¢ Weekly dashboard review with stakeholders\",\n",
    "            \"\",\n",
    "            \"Tools: MLflow, Evidently AI, or custom dashboard\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': '3. SHORT-TERM: Optimize Offer Design for Fairness (Month 1-2)',\n",
    "        'priority': 'HIGH',\n",
    "        'items': [\n",
    "            \"From SHAP: Offer design drives 52.3% of predictions (discount 21%, duration 14%, difficulty 9%)\",\n",
    "            \"GOOD NEWS: Offer features are controllable, unlike demographics\",\n",
    "            \"\",\n",
    "            \"Actions:\",\n",
    "            \"  â€¢ Test if fairness improves when using offer-based features ONLY:\",\n",
    "            \"    - Retrain model WITHOUT demographic features\",\n",
    "            \"    - Compare: Does accuracy drop significantly?\",\n",
    "            \"    - Compare: Do fairness metrics improve?\",\n",
    "            \"  â€¢ Personalize offers by behavior, not demographics:\",\n",
    "            \"    - Use: offer_viewed, tenure, past behavior\",\n",
    "            \"    - Avoid: gender, age, income as primary drivers\",\n",
    "            \"  â€¢ Create 'universal offers' that work across all demographics:\",\n",
    "            \"    - Short duration (5-7 days)\",\n",
    "            \"    - Low difficulty (easy thresholds)\",\n",
    "            \"    - Discount type (highest completion: 54.9% vs 28.4% for others)\",\n",
    "            \"\",\n",
    "            \"Expected Impact: Reduce demographic bias while maintaining performance\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': '4. MEDIUM-TERM: Segment-Specific Strategies (Month 2-3)',\n",
    "        'priority': 'MEDIUM',\n",
    "        'items': [\n",
    "            \"From Segmentation: Clear performance tiers exist (15.7% to 72.6% completion)\",\n",
    "            \"From Fairness: Check if certain demographics are trapped in low-performing segments\",\n",
    "            \"\",\n",
    "            \"Actions for each segment:\",\n",
    "            \"\",\n",
    "            \"CLUSTER 0 (72.6% completion, 29% of customers) - HIGH PERFORMERS:\",\n",
    "            \"  â€¢ Maintain satisfaction: VIP treatment, exclusive offers\",\n",
    "            \"  â€¢ Use as fairness baseline: All groups should perform like this\",\n",
    "            \"  â€¢ Analyze: What demographics dominate this cluster?\",\n",
    "            \"\",\n",
    "            \"CLUSTER 1 (55.9% completion, 43% of customers) - LARGEST OPPORTUNITY:\",\n",
    "            \"  â€¢ Educational content: 'How to maximize rewards'\",\n",
    "            \"  â€¢ Progressive difficulty: Start easy, increase over time\",\n",
    "            \"  â€¢ Target improvement: +6,169 conversions if improved to 72.6%\",\n",
    "            \"\",\n",
    "            \"CLUSTER 2 (15.7% completion, 12% of customers) - DATA QUALITY CRISIS:\",\n",
    "            \"  â€¢ Fix missing demographics (see Recommendation #1)\",\n",
    "            \"  â€¢ Use behavioral features until demographics collected\",\n",
    "            \"  â€¢ Target improvement: +4,004 conversions\",\n",
    "            \"\",\n",
    "            \"CLUSTER 3 (39.0% completion, 16% of customers) - RE-ENGAGEMENT:\",\n",
    "            \"  â€¢ Zero view rate but 39% completion (unusual!)\",\n",
    "            \"  â€¢ Push notifications, email campaigns, in-app banners\",\n",
    "            \"  â€¢ Target improvement: +2,395 conversions\",\n",
    "            \"\",\n",
    "            \"Total Opportunity: +12,568 conversions (27.3% increase)\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': '5. MEDIUM-TERM: Fairness-Aware Modeling (Month 2-3)',\n",
    "        'priority': 'MEDIUM',\n",
    "        'items': [\n",
    "            \"IF fairness violations detected above (disparate impact < 0.8 or > 1.25):\",\n",
    "            \"\",\n",
    "            \"Option A: Reweighting\",\n",
    "            \"  â€¢ Increase sample weights for under-predicted groups during training\",\n",
    "            \"  â€¢ Decrease weights for over-predicted groups\",\n",
    "            \"  â€¢ Trade-off: May reduce overall accuracy by 1-3%\",\n",
    "            \"\",\n",
    "            \"Option B: Adversarial Debiasing\",\n",
    "            \"  â€¢ Train model to predict offers well BUT hide demographic info\",\n",
    "            \"  â€¢ Use adversarial network that tries to predict demographics from predictions\",\n",
    "            \"  â€¢ Force model to be 'blind' to protected attributes\",\n",
    "            \"\",\n",
    "            \"Option C: Post-Processing Calibration\",\n",
    "            \"  â€¢ Train separate threshold for each demographic group\",\n",
    "            \"  â€¢ Adjust prediction thresholds to equalize TPR or precision\",\n",
    "            \"  â€¢ Easiest to implement, doesn't require retraining\",\n",
    "            \"\",\n",
    "            \"Option D: Remove Demographic Features Entirely\",\n",
    "            \"  â€¢ Retrain using only: offer features, behavioral features, tenure\",\n",
    "            \"  â€¢ Test: Does PCA with 8 components maintain fairness?\",\n",
    "            \"  â€¢ From PCA: Only 1% F1 drop with 68% fewer features\",\n",
    "            \"\",\n",
    "            \"Recommended: Start with Option D (simplest), then try Option C if needed\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': '6. LONG-TERM: Governance & Documentation (Ongoing)',\n",
    "        'priority': 'ONGOING',\n",
    "        'items': [\n",
    "            \"Documentation:\",\n",
    "            \"  â€¢ Create Model Card documenting:\",\n",
    "            \"    - Intended use (offer completion prediction for marketing)\",\n",
    "            \"    - Training data demographics (% by gender, age, income)\",\n",
    "            \"    - Known biases and limitations\",\n",
    "            \"    - Fairness metrics and thresholds\",\n",
    "            \"    - Mitigation strategies implemented\",\n",
    "            \"  â€¢ Document decisions:\",\n",
    "            \"    - Why use demographics vs behavioral-only?\",\n",
    "            \"    - What fairness-accuracy trade-offs are acceptable?\",\n",
    "            \"    - Who approves fairness threshold changes?\",\n",
    "            \"\",\n",
    "            \"Governance:\",\n",
    "            \"  â€¢ Quarterly fairness audits (minimum)\",\n",
    "            \"  â€¢ Annual deep-dive analysis like this notebook\",\n",
    "            \"  â€¢ Stakeholder review board for fairness decisions\",\n",
    "            \"  â€¢ Regulatory compliance check (EEOC, GDPR, CCPA)\",\n",
    "            \"\",\n",
    "            \"Continuous Improvement:\",\n",
    "            \"  â€¢ A/B test fairness interventions\",\n",
    "            \"  â€¢ Track customer satisfaction by demographic group\",\n",
    "            \"  â€¢ Monitor complaints/feedback for bias concerns\",\n",
    "            \"  â€¢ Update model when fairness metrics degrade\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{rec['category']}\")\n",
    "    if 'priority' in rec:\n",
    "        priority_emoji = {\n",
    "            'CRITICAL': 'ðŸ”´',\n",
    "            'HIGH': 'ðŸŸ ',\n",
    "            'MEDIUM': 'ðŸŸ¡',\n",
    "            'ONGOING': 'ðŸ”µ'\n",
    "        }.get(rec['priority'], '')\n",
    "        print(f\"{priority_emoji} Priority: {rec['priority']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for item in rec['items']:\n",
    "        if item == \"\":\n",
    "            print()\n",
    "        elif item.startswith(\"  \"):\n",
    "            print(item)\n",
    "        else:\n",
    "            print(f\"{item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: PRIORITIZED ACTION PLAN\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ”´ WEEK 1 (Critical):\")\n",
    "print(\"  â†’ Fix data collection for Cluster 2 (missing demographics)\")\n",
    "print(\"  â†’ Review fairness metrics computed above\")\n",
    "print(\"  â†’ Identify groups with >10% disparity\")\n",
    "print(\"\\nðŸŸ  MONTH 1 (High):\")\n",
    "print(\"  â†’ Set up fairness monitoring dashboard\")\n",
    "print(\"  â†’ Test model WITHOUT demographic features\")\n",
    "print(\"  â†’ Optimize offer design (short, easy, discount)\")\n",
    "print(\"\\nðŸŸ¡ MONTHS 2-3 (Medium):\")\n",
    "print(\"  â†’ Implement segment-specific strategies\")\n",
    "print(\"  â†’ Apply fairness-aware modeling if needed\")\n",
    "print(\"  â†’ A/B test interventions\")\n",
    "print(\"\\nðŸ”µ ONGOING:\")\n",
    "print(\"  â†’ Quarterly fairness audits\")\n",
    "print(\"  â†’ Model cards and documentation\")\n",
    "print(\"  â†’ Stakeholder reviews\")\n",
    "print(\"\\nExpected Total Impact: +12,568 conversions (27.3% increase) with improved fairness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../Cafe_Rewards_Offers/fairness_analysis', exist_ok=True)\n",
    "\n",
    "gender_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/gender_fairness.csv', index=False)\n",
    "age_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/age_fairness.csv', index=False)\n",
    "income_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/income_fairness.csv', index=False)\n",
    "tenure_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/tenure_fairness.csv', index=False)\n",
    "intersectional_df.to_csv('../Cafe_Rewards_Offers/fairness_analysis/intersectional_fairness.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ FAIRNESS ANALYSIS RESULTS SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - gender_fairness.csv\")\n",
    "print(\"  - age_fairness.csv\")\n",
    "print(\"  - income_fairness.csv\")\n",
    "print(\"  - tenure_fairness.csv\")\n",
    "print(\"  - intersectional_fairness.csv\")\n",
    "print(\"\\nLocation: ../Cafe_Rewards_Offers/fairness_analysis/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive bias and fairness analysis of the offer completion prediction model, integrated with findings from all previous analyses.\n",
    "\n",
    "### What Was Analyzed:\n",
    "1. **Protected Attributes**: Gender, Age Group, Income Bracket, Tenure Group\n",
    "2. **Fairness Metrics**: Accuracy, Precision, Recall, F1-Score, Positive Rate, TPR, FPR, AUC\n",
    "3. **Bias Types**: Demographic parity, equal opportunity, predictive parity\n",
    "4. **Intersectional Analysis**: Combinations of protected attributes\n",
    "\n",
    "### Fairness Frameworks Applied:\n",
    "- **Demographic Parity (80% Rule)**: Disparate impact between 0.8-1.25\n",
    "- **Equal Opportunity**: Similar true positive rates across groups\n",
    "- **Predictive Parity**: Similar precision across groups\n",
    "- **Individual Fairness**: Similar predictions for similar individuals\n",
    "\n",
    "### Key Findings from Integrated Analysis:\n",
    "\n",
    "#### 1. Model Design Creates Inherent Bias Risk\n",
    "From SHAP analysis, we know:\n",
    "- **Demographics drive 33.5% of predictions** (gender 5.9%, income 5.7%, age 5.3%)\n",
    "- **Offer design drives 52.3%** (type, duration, difficulty)\n",
    "- This means demographic features ARE used significantly in predictions\n",
    "- **Risk**: Groups with different demographic distributions will receive different prediction rates\n",
    "\n",
    "#### 2. Segmentation Reveals Systematic Disadvantage\n",
    "From customer segmentation:\n",
    "- **Cluster 2 (12% of customers)** has missing demographics and **15.7% completion rate**\n",
    "- Best-performing Cluster 0 has **72.6% completion rate** (4.6x better)\n",
    "- If certain demographic groups are overrepresented in low-performing clusters, they face systematic disadvantage\n",
    "- **Critical**: The fairness analysis in this notebook should reveal if specific groups are concentrated in poor-performing clusters\n",
    "\n",
    "#### 3. Data Quality Drives Fairness\n",
    "- Segmentation identified 9,963 customers (12%) with missing demographics\n",
    "- These customers perform **5x worse** than high performers\n",
    "- If demographic data collection is biased (e.g., certain groups less likely to provide info), this creates unfairness\n",
    "- **Action**: Audit data collection process for demographic bias\n",
    "\n",
    "#### 4. Feature Importance Reveals Bias Mechanisms\n",
    "SHAP showed offer design matters more than demographics (52% vs 34%), which is GOOD for fairness:\n",
    "- Offer design features (discount, duration, difficulty) are **controllable**\n",
    "- Demographic features are **immutable** (can't change age, income overnight)\n",
    "- **Opportunity**: Improve fairness by optimizing offer design rather than relying on demographics\n",
    "\n",
    "### Critical Questions Answered by This Analysis:\n",
    "\n",
    "The cells above should have revealed:\n",
    "\n",
    "1. **Are certain demographic groups systematically under-predicted or over-predicted?**\n",
    "   - Check gender, age, and income fairness metrics\n",
    "   - Disparate impact ratios should be between 0.8-1.25\n",
    "   \n",
    "2. **Do high-performing customer segments over-represent certain demographics?**\n",
    "   - Cross-reference segment membership with protected attributes\n",
    "   - If Cluster 0 (72.6% completion) is predominantly high-income males, this creates bias\n",
    "   \n",
    "3. **Are fairness violations due to model bias or real behavioral differences?**\n",
    "   - If SHAP shows demographics are important AND fairness metrics show disparities, likely model bias\n",
    "   - If demographics aren't important but disparities exist, likely real behavioral differences\n",
    "   \n",
    "4. **Which groups need targeted interventions?**\n",
    "   - Groups with high FPR: receiving offers they won't complete (wasted marketing spend)\n",
    "   - Groups with high FNR: missing offers they would complete (lost revenue)\n",
    "   - Groups in low-performing segments: need re-engagement strategies\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "#### Immediate Actions (Week 1-2):\n",
    "1. **Review the fairness metrics computed above**\n",
    "   - Identify any group with >10% disparity in accuracy, precision, or recall\n",
    "   - Flag any disparate impact violations (ratio < 0.8 or > 1.25)\n",
    "   \n",
    "2. **Cross-reference with segmentation results**\n",
    "   - Compute demographic distribution across customer segments\n",
    "   - Identify if certain groups are trapped in low-performing segments\n",
    "   \n",
    "3. **Audit data collection for demographic bias**\n",
    "   - Why do 12% of customers have missing demographics?\n",
    "   - Are certain groups less likely to provide this information?\n",
    "\n",
    "#### Short-term (Month 1-2):\n",
    "1. **If bias is detected:**\n",
    "   - Consider fairness-aware algorithms (e.g., reweighting, adversarial debiasing)\n",
    "   - Apply post-processing calibration to equalize predictions across groups\n",
    "   - Test model performance with/without demographic features\n",
    "   \n",
    "2. **If bias is minimal:**\n",
    "   - Document findings and create model cards\n",
    "   - Set up ongoing fairness monitoring in production\n",
    "   - Establish thresholds for acceptable disparities\n",
    "\n",
    "#### Long-term (Quarter 1-2):\n",
    "1. **Implement fairness monitoring dashboard**\n",
    "   - Track fairness metrics in real-time by protected attribute\n",
    "   - Alert when disparities exceed thresholds\n",
    "   - Regular audits (quarterly minimum)\n",
    "   \n",
    "2. **Balance business objectives with fairness**\n",
    "   - Define acceptable fairness-accuracy trade-offs\n",
    "   - Document decisions and rationale\n",
    "   - Engage stakeholders in fairness discussions\n",
    "   \n",
    "3. **Improve data quality**\n",
    "   - Fix onboarding to collect complete demographics\n",
    "   - Offer incentives for profile completion\n",
    "   - Reduce Cluster 2 (missing demographics) to <5% of customers\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "âœ… **The model DOES use demographics** (33.5% of prediction importance)\n",
    "âœ… **Customer segments show 4.6x performance gap** (15.7% to 72.6%)\n",
    "âœ… **12% of customers have missing demographics** and perform poorly\n",
    "âœ… **The fairness analysis above reveals if bias exists** across protected groups\n",
    "âš ï¸ **Action required**: Review all fairness metrics and implement mitigation strategies\n",
    "\n",
    "**Remember**: Fairness is context-dependent and requires ongoing monitoring. This analysis provides a snapshot; production deployment requires continuous evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_beansage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
