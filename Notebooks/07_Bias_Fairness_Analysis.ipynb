{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Bias and Fairness Analysis\n",
    "\n",
    "**Goal**: Evaluate the fairness of the offer completion prediction model across different demographic groups.\n",
    "\n",
    "**Why Fairness Matters:**\n",
    "- Avoid discriminatory outcomes in marketing offers\n",
    "- Ensure equitable customer experience\n",
    "- Build trust in AI-driven recommendations\n",
    "- Meet regulatory and ethical standards\n",
    "\n",
    "**Protected Attributes Analyzed:**\n",
    "1. **Gender**: Male, Female, Other, Missing\n",
    "2. **Age Group**: 18-30, 31-45, 46-60, 61-75, 76+\n",
    "3. **Income Bracket**: Missing, Low, Medium, High, Very High\n",
    "\n",
    "**Fairness Metrics:**\n",
    "- **Demographic Parity**: Similar prediction rates across groups\n",
    "- **Equal Opportunity**: Similar true positive rates across groups\n",
    "- **Predictive Parity**: Similar precision across groups\n",
    "- **Disparate Impact**: Ratio of favorable outcomes between groups\n",
    "- **Overall Accuracy**: Similar accuracy across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready! ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, confusion_matrix, roc_auc_score,\n",
    "                            precision_recall_curve, roc_curve)\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Environment ready! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "load_processed_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded: 17,287 samples √ó 26 features\n",
      "Target distribution in test set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "processed_dir = '../Cafe_Rewards_Offers/processed'\n",
    "models_dir = '../Cafe_Rewards_Offers/models'\n",
    "\n",
    "X_test = joblib.load(f'{processed_dir}/X_test_scaled.pkl')\n",
    "y_test = joblib.load(f'{processed_dir}/y_test.pkl')\n",
    "feature_names = joblib.load(f'{processed_dir}/feature_names.pkl')\n",
    "\n",
    "print(f\"Test set loaded: {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")\n",
    "print(f\"Target distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1e3c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FOR DATA LEAKAGE\n",
      "============================================================\n",
      "\n",
      "Features (26 total):\n",
      "   0. received_time\n",
      "   1. difficulty\n",
      "   2. duration\n",
      "   3. in_email\n",
      "   4. in_mobile\n",
      "   5. in_social\n",
      "   6. in_web\n",
      "   7. offer_received\n",
      "   8. offer_viewed\n",
      "   9. offer_completed\n",
      "  10. age\n",
      "  11. income\n",
      "  12. membership_year\n",
      "  13. is_demographics_missing\n",
      "  14. membership_duration_days\n",
      "  15. membership_month\n",
      "  16. offer_type_bogo\n",
      "  17. offer_type_discount\n",
      "  18. offer_type_informational\n",
      "  19. gender_F\n",
      "  20. gender_M\n",
      "  21. gender_Missing\n",
      "  22. gender_O\n",
      "  23. age_group_encoded\n",
      "  24. income_bracket_encoded\n",
      "  25. tenure_group_encoded\n",
      "\n",
      "============================================================\n",
      "CHECKING FOR PERFECT CORRELATION\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Combine X and y for correlation check\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m train_df = \u001b[43mX_train\u001b[49m.copy()\n\u001b[32m     17\u001b[39m train_df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m] = y_train.values\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Calculate correlation with target\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CHECKING FOR DATA LEAKAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check feature names\n",
    "print(f\"\\nFeatures ({len(feature_names)} total):\")\n",
    "for i, feat in enumerate(feature_names):\n",
    "    print(f\"  {i:2}. {feat}\")\n",
    "\n",
    "# Check if target has perfect correlation with any feature\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING FOR PERFECT CORRELATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine X and y for correlation check\n",
    "train_df = X_train.copy()\n",
    "train_df['target'] = y_train.values\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = train_df.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop correlations with target:\")\n",
    "for feat, corr in correlations.head(10).items():\n",
    "    print(f\"  {feat:30}: {corr:.4f}\")\n",
    "\n",
    "# Flag potential data leaks (correlation = 1.0 or near 1.0)\n",
    "perfect_leaks = correlations[correlations == 1.0]\n",
    "if len(perfect_leaks) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  DATA LEAKAGE DETECTED!\")\n",
    "    print(f\"Features with perfect correlation (r=1.0):\")\n",
    "    for feat in perfect_leaks.index:\n",
    "        print(f\"  - {feat}\")\n",
    "    print(\"\\n‚ö†Ô∏è  ACTION REQUIRED: Remove these features before modeling!\")\n",
    "else:\n",
    "    print(\"\\n‚úì No perfect data leaks detected (correlation < 1.0)\")\n",
    "\n",
    "# Check for near-perfect leaks (correlation > 0.95)\n",
    "near_leaks = correlations[(correlations > 0.95) & (correlations < 1.0)]\n",
    "if len(near_leaks) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  NEAR-PERFECT DATA LEAKAGE DETECTED!\")\n",
    "    print(f\"Features with near-perfect correlation (r > 0.95):\")\n",
    "    for feat, corr in near_leaks.items():\n",
    "        print(f\"  - {feat:30}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects 25 features:\n",
      "  Test set has 26 features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- offer_completed\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m         rf_tuned.fit(X_train, y_train)\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚úì Model retrained without data leakage\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m y_pred = \u001b[43mrf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m y_proba = rf_model.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRandom Forest model loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_Projects/venv_beansage/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_Projects/venv_beansage/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_Projects/venv_beansage/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_Projects/venv_beansage/lib/python3.12/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Python_Projects/venv_beansage/lib/python3.12/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- offer_completed\n"
     ]
    }
   ],
   "source": [
    "rf_model = joblib.load(f'{models_dir}/random_forest.pkl')\n",
    "rf_tuned = joblib.load(f'{models_dir}/random_forest_tuned.pkl')\n",
    "\n",
    "print(f\"Model expects {len(rf_model.feature_names_in_)} features:\")\n",
    "print(f\"  Test set has {X_test.shape[1]} features\")\n",
    "\n",
    "if 'offer_completed' in rf_model.feature_names_in_:\n",
    "    print(\"\\n‚ö†Ô∏è  Model was trained with 'offer_completed' (data leakage)\")\n",
    "    print(\"   Retraining model without leakage feature...\")\n",
    "    \n",
    "    X_train = joblib.load(f'{processed_dir}/X_train_scaled.pkl')\n",
    "    y_train = joblib.load(f'{processed_dir}/y_train.pkl')\n",
    "    \n",
    "    if 'offer_completed' in X_train.columns:\n",
    "        X_train = X_train.drop('offer_completed', axis=1)\n",
    "        X_test = X_test.drop('offer_completed', axis=1)\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_tuned.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"   ‚úì Model retrained without data leakage\")\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nRandom Forest model loaded!\")\n",
    "print(f\"\\nOverall Model Performance on Test Set:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y_test, y_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected_attributes",
   "metadata": {},
   "source": [
    "## Protected Attributes Analysis\n",
    "\n",
    "We'll analyze model performance across protected attributes from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_original_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../Cafe_Rewards_Offers/processed_data_for_classification.csv')\n",
    "\n",
    "print(f\"Original dataset loaded: {df_original.shape[0]:,} rows √ó {df_original.shape[1]} columns\")\n",
    "print(f\"\\nColumns available for fairness analysis:\")\n",
    "protected_cols = ['gender', 'age', 'income', 'age_group', 'income_bracket', 'tenure_group']\n",
    "for col in protected_cols:\n",
    "    if col in df_original.columns:\n",
    "        unique_vals = df_original[col].unique()\n",
    "        print(f\"  - {col}: {len(unique_vals)} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_protected_attributes",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fairness = X_test.copy()\n",
    "df_test_fairness['target'] = y_test.values\n",
    "df_test_fairness['prediction'] = y_pred\n",
    "df_test_fairness['prediction_proba'] = y_proba\n",
    "\n",
    "df_original_test = df_original.iloc[X_test.index].copy()\n",
    "\n",
    "for col in ['gender', 'age_group', 'income_bracket', 'tenure_group']:\n",
    "    if col in df_original_test.columns:\n",
    "        df_test_fairness[col] = df_original_test[col].values\n",
    "\n",
    "print(f\"Fairness analysis dataframe created: {df_test_fairness.shape}\")\n",
    "print(f\"\\nProtected attributes added: {['gender', 'age_group', 'income_bracket', 'tenure_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fairness_functions",
   "metadata": {},
   "source": [
    "## Fairness Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_group_metrics(y_true, y_pred, y_proba, group_mask):\n",
    "    \"\"\"Calculate classification metrics for a specific subgroup.\"\"\"\n",
    "    if sum(group_mask) < 10:\n",
    "        return None\n",
    "    \n",
    "    y_true_g = y_true[group_mask]\n",
    "    y_pred_g = y_pred[group_mask]\n",
    "    y_proba_g = y_proba[group_mask]\n",
    "    \n",
    "    cm = confusion_matrix(y_true_g, y_pred_g)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    metrics = {\n",
    "        'count': sum(group_mask),\n",
    "        'positive_rate': y_pred_g.mean(),\n",
    "        'accuracy': accuracy_score(y_true_g, y_pred_g),\n",
    "        'precision': precision_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'recall': recall_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'f1': f1_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'tpr': recall_score(y_true_g, y_pred_g, zero_division=0),\n",
    "        'tnr': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        'fnr': fn / (fn + tp) if (fn + tp) > 0 else 0,\n",
    "        'auc': roc_auc_score(y_true_g, y_proba_g) if len(np.unique(y_true_g)) > 1 else np.nan\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def analyze_fairness_by_attribute(df, attribute, y_true_col='target', \n",
    "                                  y_pred_col='prediction', y_proba_col='prediction_proba'):\n",
    "    \"\"\"Analyze fairness metrics across all values of a protected attribute.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    overall_metrics = calculate_group_metrics(\n",
    "        df[y_true_col].values, \n",
    "        df[y_pred_col].values, \n",
    "        df[y_proba_col].values,\n",
    "        np.ones(len(df), dtype=bool)\n",
    "    )\n",
    "    \n",
    "    for value in df[attribute].unique():\n",
    "        if pd.isna(value):\n",
    "            continue\n",
    "        \n",
    "        mask = df[attribute] == value\n",
    "        group_metrics = calculate_group_metrics(\n",
    "            df[y_true_col].values, \n",
    "            df[y_pred_col].values, \n",
    "            df[y_proba_col].values,\n",
    "            mask\n",
    "        )\n",
    "        \n",
    "        if group_metrics:\n",
    "            group_metrics['attribute'] = attribute\n",
    "            group_metrics['value'] = value\n",
    "            \n",
    "            for metric in ['accuracy', 'precision', 'recall', 'f1', 'positive_rate', 'tpr', 'fpr']:\n",
    "                if overall_metrics[metric] > 0:\n",
    "                    diff = group_metrics[metric] - overall_metrics[metric]\n",
    "                    group_metrics[f'{metric}_diff'] = diff\n",
    "                    if overall_metrics[metric] > 0:\n",
    "                        group_metrics[f'{metric}_pct_diff'] = (diff / overall_metrics[metric]) * 100\n",
    "            \n",
    "            results.append(group_metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def calculate_disparate_impact(df, attribute, y_pred_col='prediction', reference_value=None):\n",
    "    \"\"\"Calculate disparate impact ratio for a protected attribute.\"\"\"\n",
    "    \n",
    "    positive_rates = df.groupby(attribute)[y_pred_col].mean()\n",
    "    \n",
    "    if reference_value is None:\n",
    "        reference_value = positive_rates.idxmax()\n",
    "    \n",
    "    reference_rate = positive_rates[reference_value]\n",
    "    \n",
    "    di_results = []\n",
    "    for value, rate in positive_rates.items():\n",
    "        if reference_rate > 0:\n",
    "            di = rate / reference_rate\n",
    "        else:\n",
    "            di = np.nan\n",
    "        \n",
    "        di_results.append({\n",
    "            'attribute': attribute,\n",
    "            'value': value,\n",
    "            'positive_rate': rate,\n",
    "            'reference': reference_value,\n",
    "            'reference_rate': reference_rate,\n",
    "            'disparate_impact': di,\n",
    "            'is_fair': 0.8 <= di <= 1.25\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(di_results)\n",
    "\n",
    "\n",
    "def plot_fairness_comparison(metrics_df, attribute, metric_cols=['accuracy', 'precision', 'recall', 'f1']):\n",
    "    \"\"\"Plot fairness metrics comparison across groups.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Fairness Analysis by {attribute}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values('value')\n",
    "    values = metrics_df['value'].values\n",
    "    \n",
    "    for idx, metric in enumerate(metric_cols):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        bars = ax.bar(values, metrics_df[metric].values, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        overall_mean = metrics_df[f'{metric}_diff'].mean() + metrics_df[metric].mean()\n",
    "        ax.axhline(y=overall_mean, color='red', linestyle='--', linewidth=2, \n",
    "                  label=f'Overall Mean: {overall_mean:.3f}')\n",
    "        \n",
    "        for bar, val in zip(bars, metrics_df[metric].values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax.set_xlabel(attribute)\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} by {attribute}')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_positive_rates(metrics_df, attribute):\n",
    "    \"\"\"Plot positive prediction rates across groups (Demographic Parity).\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values('positive_rate')\n",
    "    \n",
    "    colors = ['green' if 0.8 <= (rate / metrics_df['positive_rate'].max()) <= 1.25 \n",
    "              else 'orange' for rate in metrics_df['positive_rate']]\n",
    "    \n",
    "    bars = plt.bar(metrics_df['value'], metrics_df['positive_rate'], \n",
    "                    color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.axhline(y=metrics_df['positive_rate'].mean(), color='red', \n",
    "                linestyle='--', linewidth=2, \n",
    "                label=f'Overall Mean: {metrics_df[\"positive_rate\"].mean():.3f}')\n",
    "    \n",
    "    for bar, val in zip(bars, metrics_df['positive_rate'].values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.xlabel(attribute)\n",
    "    plt.ylabel('Positive Prediction Rate')\n",
    "    plt.title(f'Demographic Parity - Positive Rate by {attribute}', \n",
    "              fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Fairness metrics functions defined! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gender_analysis",
   "metadata": {},
   "source": [
    "## 1. Gender-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GENDER DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gender_dist = df_test_fairness['gender'].value_counts(normalize=True).sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for gender, pct in gender_dist.items():\n",
    "    count = (df_test_fairness['gender'] == gender).sum()\n",
    "    print(f\"  {gender}: {count:5,} ({pct*100:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by gender:\")\n",
    "for gender in df_test_fairness['gender'].unique():\n",
    "    if pd.notna(gender):\n",
    "        subset = df_test_fairness[df_test_fairness['gender'] == gender]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {gender}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_fairness = analyze_fairness_by_attribute(df_test_fairness, 'gender')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENDER FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(gender_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(gender_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_di = calculate_disparate_impact(df_test_fairness, 'gender')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENDER DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 ‚â§ DI ‚â§ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in gender_di.iterrows():\n",
    "    status = \"‚úì FAIR\" if row['is_fair'] else \"‚ö†Ô∏è  UNFAIR\"\n",
    "    print(f\"{row['value']:10} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gender_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(gender_fairness, 'gender')\n",
    "plot_positive_rates(gender_fairness, 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "age_analysis",
   "metadata": {},
   "source": [
    "## 2. Age Group-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "age_dist = df_test_fairness['age_group'].value_counts().sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for age, count in age_dist.items():\n",
    "    if pd.notna(age):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {age:10}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by age group:\")\n",
    "for age in sorted(df_test_fairness['age_group'].unique()):\n",
    "    if pd.notna(age):\n",
    "        subset = df_test_fairness[df_test_fairness['age_group'] == age]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {age:10}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_fairness = analyze_fairness_by_attribute(df_test_fairness, 'age_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(age_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(age_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_di = calculate_disparate_impact(df_test_fairness, 'age_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGE GROUP DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 ‚â§ DI ‚â§ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in age_di.iterrows():\n",
    "    status = \"‚úì FAIR\" if row['is_fair'] else \"‚ö†Ô∏è  UNFAIR\"\n",
    "    print(f\"{row['value']:10} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(age_fairness, 'age_group')\n",
    "plot_positive_rates(age_fairness, 'age_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "income_analysis",
   "metadata": {},
   "source": [
    "## 3. Income Bracket-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "income_dist = df_test_fairness['income_bracket'].value_counts().sort_index()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for income, count in income_dist.items():\n",
    "    if pd.notna(income):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {income:12}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by income bracket:\")\n",
    "for income in sorted(df_test_fairness['income_bracket'].unique()):\n",
    "    if pd.notna(income):\n",
    "        subset = df_test_fairness[df_test_fairness['income_bracket'] == income]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {income:12}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_fairness = analyze_fairness_by_attribute(df_test_fairness, 'income_bracket')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(income_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(income_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_disparate_impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_di = calculate_disparate_impact(df_test_fairness, 'income_bracket')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INCOME BRACKET DISPARATE IMPACT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDisparate Impact Ratio = (Group Positive Rate) / (Reference Group Positive Rate)\")\n",
    "print(\"Fair range: 0.8 ‚â§ DI ‚â§ 1.25 (80% rule)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for _, row in income_di.iterrows():\n",
    "    status = \"‚úì FAIR\" if row['is_fair'] else \"‚ö†Ô∏è  UNFAIR\"\n",
    "    print(f\"{row['value']:12} | Rate: {row['positive_rate']:.4f} | DI: {row['disparate_impact']:.3f} | {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "income_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(income_fairness, 'income_bracket')\n",
    "plot_positive_rates(income_fairness, 'income_bracket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tenure_analysis",
   "metadata": {},
   "source": [
    "## 4. Tenure Group-based Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TENURE GROUP DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tenure_dist = df_test_fairness['tenure_group'].value_counts()\n",
    "print(\"\\nTest set distribution:\")\n",
    "for tenure, count in tenure_dist.items():\n",
    "    if pd.notna(tenure):\n",
    "        pct = (count / len(df_test_fairness)) * 100\n",
    "        print(f\"  {tenure:15}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nTarget completion rate by tenure group:\")\n",
    "for tenure in df_test_fairness['tenure_group'].unique():\n",
    "    if pd.notna(tenure):\n",
    "        subset = df_test_fairness[df_test_fairness['tenure_group'] == tenure]\n",
    "        completion_rate = subset['target'].mean()\n",
    "        print(f\"  {tenure:15}: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_fairness_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_fairness = analyze_fairness_by_attribute(df_test_fairness, 'tenure_group')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TENURE GROUP FAIRNESS METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "display_cols = ['value', 'count', 'accuracy', 'precision', 'recall', 'f1', \n",
    "                'positive_rate', 'tpr', 'fpr', 'auc']\n",
    "print(tenure_fairness[display_cols].round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISPARITIES FROM OVERALL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diff_cols = ['value', 'accuracy_pct_diff', 'precision_pct_diff', \n",
    "             'recall_pct_diff', 'f1_pct_diff']\n",
    "print(tenure_fairness[diff_cols].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenure_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_comparison(tenure_fairness, 'tenure_group')\n",
    "plot_positive_rates(tenure_fairness, 'tenure_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrices",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrices by Protected Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_confusion_matrix(y_true, y_pred, group_name, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n",
    "                xticklabels=['Not Completed', 'Completed'],\n",
    "                yticklabels=['Not Completed', 'Completed'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(group_name, fontweight='bold')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices by Gender', fontsize=16, fontweight='bold')\n",
    "\n",
    "gender_values = [g for g in df_test_fairness['gender'].unique() if pd.notna(g)]\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, gender in enumerate(gender_values):\n",
    "    mask = df_test_fairness['gender'] == gender\n",
    "    plot_group_confusion_matrix(\n",
    "        df_test_fairness[mask]['target'],\n",
    "        df_test_fairness[mask]['prediction'],\n",
    "        f'Gender: {gender} (n={mask.sum():,})',\n",
    "        axes_flat[idx]\n",
    "    )\n",
    "\n",
    "for idx in range(len(gender_values), len(axes_flat)):\n",
    "    axes_flat[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "age_confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Confusion Matrices by Age Group', fontsize=16, fontweight='bold')\n",
    "\n",
    "age_values = sorted([a for a in df_test_fairness['age_group'].unique() if pd.notna(a)])\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, age in enumerate(age_values):\n",
    "    mask = df_test_fairness['age_group'] == age\n",
    "    plot_group_confusion_matrix(\n",
    "        df_test_fairness[mask]['target'],\n",
    "        df_test_fairness[mask]['prediction'],\n",
    "        f'Age: {age} (n={mask.sum():,})',\n",
    "        axes_flat[idx]\n",
    "    )\n",
    "\n",
    "for idx in range(len(age_values), len(axes_flat)):\n",
    "    axes_flat[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc_curves",
   "metadata": {},
   "source": [
    "## 6. ROC Curves by Protected Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_roc_by_groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_by_group(df, attribute, title_suffix):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    values = sorted([v for v in df[attribute].unique() if pd.notna(v)])\n",
    "    \n",
    "    for value in values:\n",
    "        mask = df[attribute] == value\n",
    "        if sum(mask) < 10:\n",
    "            continue\n",
    "        \n",
    "        y_true = df[mask]['target']\n",
    "        y_proba = df[mask]['prediction_proba']\n",
    "        \n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            continue\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        auc_score = roc_auc_score(y_true, y_proba)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f'{value} (AUC = {auc_score:.3f})', linewidth=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves by {title_suffix}', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_by_group(df_test_fairness, 'gender', 'Gender')\n",
    "plot_roc_by_group(df_test_fairness, 'age_group', 'Age Group')\n",
    "plot_roc_by_group(df_test_fairness, 'income_bracket', 'Income Bracket')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intersectional",
   "metadata": {},
   "source": [
    "## 7. Intersectional Fairness Analysis\n",
    "\n",
    "Analyze fairness across intersections of protected attributes (e.g., Gender √ó Age Group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intersectional_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INTERSECTIONAL FAIRNESS: GENDER √ó AGE GROUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "intersectional_results = []\n",
    "\n",
    "for gender in ['M', 'F']:\n",
    "    for age in sorted([a for a in df_test_fairness['age_group'].unique() if pd.notna(a)]):\n",
    "        mask = (df_test_fairness['gender'] == gender) & (df_test_fairness['age_group'] == age)\n",
    "        \n",
    "        if sum(mask) < 10:\n",
    "            continue\n",
    "        \n",
    "        y_true = df_test_fairness[mask]['target']\n",
    "        y_pred = df_test_fairness[mask]['prediction']\n",
    "        \n",
    "        metrics = {\n",
    "            'gender': gender,\n",
    "            'age_group': age,\n",
    "            'count': sum(mask),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'positive_rate': y_pred.mean()\n",
    "        }\n",
    "        \n",
    "        intersectional_results.append(metrics)\n",
    "\n",
    "intersectional_df = pd.DataFrame(intersectional_results)\n",
    "\n",
    "print(\"\\nIntersectional Fairness Metrics:\")\n",
    "print(intersectional_df.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MAXIMUM DISPARITY IN INTERSECTIONAL GROUPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'positive_rate']:\n",
    "    if len(intersectional_df) > 0:\n",
    "        max_val = intersectional_df[metric].max()\n",
    "        min_val = intersectional_df[metric].min()\n",
    "        disparity = max_val - min_val\n",
    "        print(f\"{metric:15}: Max={max_val:.3f}, Min={min_val:.3f}, Disparity={disparity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intersectional_heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_accuracy = intersectional_df.pivot(index='age_group', columns='gender', values='accuracy')\n",
    "pivot_recall = intersectional_df.pivot(index='age_group', columns='gender', values='recall')\n",
    "pivot_f1 = intersectional_df.pivot(index='age_group', columns='gender', values='f1')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.heatmap(pivot_accuracy, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'Accuracy'}, ax=axes[0])\n",
    "axes[0].set_title('Accuracy by Gender √ó Age', fontweight='bold')\n",
    "\n",
    "sns.heatmap(pivot_recall, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'Recall'}, ax=axes[1])\n",
    "axes[1].set_title('Recall by Gender √ó Age', fontweight='bold')\n",
    "\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn', cbar_kws={'label': 'F1-Score'}, ax=axes[2])\n",
    "axes[2].set_title('F1-Score by Gender √ó Age', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fairness_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fairness_summary(gender_fairness, age_fairness, income_fairness, tenure_fairness):\n",
    "    \"\"\"Generate a comprehensive fairness summary.\"\"\"\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    fairness_dfs = {\n",
    "        'gender': gender_fairness,\n",
    "        'age_group': age_fairness,\n",
    "        'income_bracket': income_fairness,\n",
    "        'tenure_group': tenure_fairness\n",
    "    }\n",
    "    \n",
    "    for attr_name, df in fairness_dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        metrics_to_check = ['accuracy', 'precision', 'recall', 'f1', 'positive_rate']\n",
    "        \n",
    "        summary[attr_name] = {}\n",
    "        \n",
    "        for metric in metrics_to_check:\n",
    "            if f'{metric}_pct_diff' in df.columns:\n",
    "                max_diff = df[f'{metric}_pct_diff'].abs().max()\n",
    "                min_diff = df[f'{metric}_pct_diff'].abs().min()\n",
    "                \n",
    "                if max_diff > 10:\n",
    "                    risk_level = \"HIGH\"\n",
    "                elif max_diff > 5:\n",
    "                    risk_level = \"MEDIUM\"\n",
    "                else:\n",
    "                    risk_level = \"LOW\"\n",
    "                \n",
    "                summary[attr_name][metric] = {\n",
    "                    'max_disparity_pct': round(max_diff, 2),\n",
    "                    'risk_level': risk_level\n",
    "                }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "fairness_summary = generate_fairness_summary(\n",
    "    gender_fairness, age_fairness, income_fairness, tenure_fairness\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FAIRNESS ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for attr_name, metrics in fairness_summary.items():\n",
    "    print(f\"\\n{attr_name.upper()}:\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        risk_emoji = {\n",
    "            'HIGH': 'üî¥',\n",
    "            'MEDIUM': 'üü°',\n",
    "            'LOW': 'üü¢'\n",
    "        }[values['risk_level']]\n",
    "        print(f\"  {metric_name:15}: {values['max_disparity_pct']:>6.2f}% disparity - {risk_emoji} {values['risk_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = [\n",
    "    {\n",
    "        'category': 'Model Monitoring',\n",
    "        'items': [\n",
    "            \"Implement ongoing fairness monitoring in production\",\n",
    "            \"Set up alerts for fairness metric degradation > 5%\",\n",
    "            \"Regularly audit model performance across protected groups\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Data Collection',\n",
    "        'items': [\n",
    "            \"Ensure balanced representation across all groups\",\n",
    "            \"Collect more data for underrepresented groups\",\n",
    "            \"Address missing demographic data systematically\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Mitigation Strategies',\n",
    "        'items': [\n",
    "            \"Consider fairness-aware algorithms if disparities are high\",\n",
    "            \"Apply post-processing techniques to calibrate predictions\",\n",
    "            \"Use reweighting strategies during training\",\n",
    "            \"Test with/without sensitive attributes to understand bias sources\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Business Context',\n",
    "        'items': [\n",
    "            \"Define acceptable fairness thresholds for your use case\",\n",
    "            \"Balance fairness with business objectives\",\n",
    "            \"Document trade-offs between accuracy and fairness\",\n",
    "            \"Engage stakeholders in fairness decisions\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'category': 'Governance',\n",
    "        'items': [\n",
    "            \"Document fairness analysis process and results\",\n",
    "            \"Create model cards documenting biases and limitations\",\n",
    "            \"Establish fairness review process before deployment\",\n",
    "            \"Consider regulatory requirements (e.g., EEOC guidelines)\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"\\n{rec['category']}:\")\n",
    "    for item in rec['items']:\n",
    "        print(f\"  ‚Ä¢ {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../Cafe_Rewards_Offers/fairness_analysis', exist_ok=True)\n",
    "\n",
    "gender_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/gender_fairness.csv', index=False)\n",
    "age_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/age_fairness.csv', index=False)\n",
    "income_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/income_fairness.csv', index=False)\n",
    "tenure_fairness.to_csv('../Cafe_Rewards_Offers/fairness_analysis/tenure_fairness.csv', index=False)\n",
    "intersectional_df.to_csv('../Cafe_Rewards_Offers/fairness_analysis/intersectional_fairness.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úì FAIRNESS ANALYSIS RESULTS SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - gender_fairness.csv\")\n",
    "print(\"  - age_fairness.csv\")\n",
    "print(\"  - income_fairness.csv\")\n",
    "print(\"  - tenure_fairness.csv\")\n",
    "print(\"  - intersectional_fairness.csv\")\n",
    "print(\"\\nLocation: ../Cafe_Rewards_Offers/fairness_analysis/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive bias and fairness analysis of the offer completion prediction model. Key takeaways:\n",
    "\n",
    "### What Was Analyzed:\n",
    "1. **Protected Attributes**: Gender, Age Group, Income Bracket, Tenure Group\n",
    "2. **Fairness Metrics**: Accuracy, Precision, Recall, F1-Score, Positive Rate, TPR, FPR, AUC\n",
    "3. **Bias Types**: Demographic parity, equal opportunity, predictive parity\n",
    "4. **Intersectional Analysis**: Combinations of protected attributes\n",
    "\n",
    "### Fairness Frameworks Applied:\n",
    "- **Demographic Parity (80% Rule)**: Disparate impact between 0.8-1.25\n",
    "- **Equal Opportunity**: Similar true positive rates across groups\n",
    "- **Predictive Parity**: Similar precision across groups\n",
    "- **Individual Fairness**: Similar predictions for similar individuals\n",
    "\n",
    "### Next Steps:\n",
    "1. Review the fairness analysis results\n",
    "2. Identify groups with high disparities (>10%)\n",
    "3. Implement appropriate mitigation strategies\n",
    "4. Set up ongoing fairness monitoring\n",
    "5. Document findings and create model cards\n",
    "\n",
    "### Key Questions to Consider:\n",
    "- Are the observed disparities acceptable for your business context?\n",
    "- Do these disparities reflect real differences in customer behavior or model bias?\n",
    "- What are the legal and ethical implications of these disparities?\n",
    "- How can you balance business objectives with fairness considerations?\n",
    "\n",
    "**Remember**: Fairness is context-dependent. What's \"fair\" depends on your specific use case, regulations, and values. Regular monitoring and iteration are essential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_beansage (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
