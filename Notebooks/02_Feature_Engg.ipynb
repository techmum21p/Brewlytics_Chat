{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**Goal**: Transform raw data into clean, model-ready features for machine learning.\n",
    "\n",
    "**Process Flow:**\n",
    "```\n",
    "Raw Data â†’ Clean â†’ Encode â†’ Impute â†’ Scale â†’ Ready for ML\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready! âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Environment ready! âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 86,432 rows Ã— 28 columns\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = '/Users/airees/Python/Brewlytics_Chat/Cafe_Rewards_Offers/processed_data_for_classification.csv'\n",
    "df = pd.read_csv(processed_dataset)\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_overview",
   "metadata": {},
   "source": [
    "## Data Overview & Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check_missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Columns with missing values:\n",
      "  completion_time: 40,280 (46.60%)\n",
      "  time_to_action: 40,280 (46.60%)\n",
      "  tenure_group: 111 (0.13%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "missing_values = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    for col, count in missing_values.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No missing values found in original dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_leakage_warning",
   "metadata": {},
   "source": [
    "## âš ï¸ Data Leakage Check\n",
    "\n",
    "**Critical Issue:** Some features would leak information about the target variable and must be removed.\n",
    "\n",
    "**Features to Drop:**\n",
    "\n",
    "1. **Perfect Leakage (Target Variable Itself):**\n",
    "   - `offer_completed` - This IS the target variable. Perfect correlation = 1.0\n",
    "   \n",
    "2. **Temporal Leakage (Not Available at Prediction Time):**\n",
    "   - `offer_viewed` - Viewing happens AFTER offer delivery, not available for real-time prediction\n",
    "   \n",
    "3. **Target-Derived Leakage:**\n",
    "   - `completion_time` - Only exists for completed offers (46.6% missing = non-completed offers)\n",
    "   - `time_to_action` - Only exists for completed offers (46.6% missing = non-completed offers)\n",
    "\n",
    "4. **Identifiers & Raw Dates (No Predictive Value):**\n",
    "   - `customer_id`, `offer_id`, `index` - Identifiers\n",
    "   - `became_member_on`, `became_member_date` - Raw date fields (replaced with derived features)\n",
    "\n",
    "**Strategy:**\n",
    "- Drop ALL leakage features (`offer_completed`, `offer_viewed`, `completion_time`, `time_to_action`)\n",
    "- Drop identifiers and redundant date columns\n",
    "- Keep only features available at offer delivery time (real-time prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drop_leakage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LEAKAGE REMOVAL\n",
      "============================================================\n",
      "\n",
      "Columns dropped: 8\n",
      "  âœ“ customer_id\n",
      "  âœ“ offer_id\n",
      "  âœ“ completion_time\n",
      "  âœ“ time_to_action\n",
      "  âœ“ offer_completed\n",
      "  âœ“ offer_viewed\n",
      "  âœ“ became_member_on\n",
      "  âœ“ became_member_date\n",
      "\n",
      "After dropping: 86,432 rows Ã— 20 columns\n",
      "\n",
      "Remaining columns (20):\n",
      "   1. received_time\n",
      "   2. difficulty\n",
      "   3. duration\n",
      "   4. offer_type\n",
      "   5. in_email\n",
      "   6. in_mobile\n",
      "   7. in_social\n",
      "   8. in_web\n",
      "   9. offer_received\n",
      "  10. target\n",
      "  11. gender\n",
      "  12. age\n",
      "  13. income\n",
      "  14. membership_year\n",
      "  15. is_demographics_missing\n",
      "  16. age_group\n",
      "  17. income_bracket\n",
      "  18. membership_duration_days\n",
      "  19. membership_month\n",
      "  20. tenure_group\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with data leakage and identifiers\n",
    "cols_to_drop = [\n",
    "    'index', 'customer_id', 'offer_id',  # Identifiers\n",
    "    'completion_time', 'time_to_action',  # Target-derived leakage\n",
    "    'offer_completed', 'offer_viewed',  # Perfect & temporal leakage\n",
    "    'became_member_on', 'became_member_date'  # Raw dates (redundant)\n",
    "]\n",
    "\n",
    "# Only drop columns that exist\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "df = df.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LEAKAGE REMOVAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nColumns dropped: {len(existing_cols_to_drop)}\")\n",
    "for col in existing_cols_to_drop:\n",
    "    print(f\"  âœ“ {col}\")\n",
    "\n",
    "print(f\"\\nAfter dropping: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nRemaining columns ({df.shape[1]}):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering_plan",
   "metadata": {},
   "source": [
    "## Feature Engineering Plan\n",
    "\n",
    "### 1. Categorical Encoding\n",
    "- `offer_type`: **One-Hot Encoding** (bogo, discount, informational)\n",
    "  - **Rationale**: No inherent order, create binary features for each category\n",
    "- `gender`: **One-Hot Encoding** (F, M, O, Missing)\n",
    "  - **Rationale**: Nominal variable with no order\n",
    "- `age_group`: **Ordinal Encoding** (18-30 â†’ 0, 31-45 â†’ 1, 46-60 â†’ 2, 61-75 â†’ 3, 76+ â†’ 4)\n",
    "  - **Rationale**: Natural ordering exists (older > younger), preserve ordinal relationship\n",
    "- `income_bracket`: **Ordinal Encoding** (Missing â†’ 0, Low â†’ 1, Medium â†’ 2, High â†’ 3, Very High â†’ 4)\n",
    "  - **Rationale**: Clear ordinal progression in income levels\n",
    "- `tenure_group`: **Ordinal Encoding** (0-6 months â†’ 0, 6-12 months â†’ 1, 1-2 years â†’ 2, 2+ years â†’ 3)\n",
    "  - **Rationale**: Chronological ordering of customer tenure\n",
    "\n",
    "### 2. Numerical Scaling\n",
    "**Features to scale**: `difficulty`, `duration`, `age`, `income`, `membership_duration_days`\n",
    "\n",
    "**Method: StandardScaler (Z-score normalization)**\n",
    "- Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- **Rationale**:\n",
    "  - Centers data around 0 (mean)\n",
    "  - Scales to unit variance (std = 1)\n",
    "  - Essential for Logistic Regression, k-NN, SVM, and gradient descent convergence\n",
    "- **Not scaling binary features** (0/1 values already on same scale)\n",
    "\n",
    "### 3. Binary Features (Already Encoded)\n",
    "- `in_email`, `in_mobile`, `in_social`, `in_web` (marketing channel flags)\n",
    "- `is_demographics_missing` (missing data indicator)\n",
    "- `offer_viewed` (action flag - potential data leak)\n",
    "\n",
    "**Rationale for leaving unchanged**: Already binary (0/1), scaling not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "encoding_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: CATEGORICAL ENCODING\n",
      "============================================================\n",
      "\n",
      "One-Hot Encoding: ['offer_type', 'gender']\n",
      "  - offer_type: 3 unique values\n",
      "    â†’ Created 3 dummy columns\n",
      "  - gender: 4 unique values\n",
      "    â†’ Created 4 dummy columns\n",
      "\n",
      "Ordinal Encoding: ['age_group', 'income_bracket', 'tenure_group']\n",
      "  - age_group: ['18-30', '31-45', '46-60', '61-75', '76+']\n",
      "    â†’ Created age_group_encoded (0-4)\n",
      "  - income_bracket: ['Missing', 'Low', 'Medium', 'High', 'Very High']\n",
      "    â†’ Created income_bracket_encoded (0-4)\n",
      "  - tenure_group: ['0-6 months', '6-12 months', '1-2 years', '2+ years']\n",
      "    â†’ Created tenure_group_encoded (0-3)\n",
      "\n",
      "After encoding: 86,432 rows Ã— 25 columns\n"
     ]
    }
   ],
   "source": [
    "df_encoded = df.copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: CATEGORICAL ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# One-Hot Encoding for nominal variables\n",
    "ohe_cols = ['offer_type', 'gender']\n",
    "print(f\"\\nOne-Hot Encoding: {ohe_cols}\")\n",
    "\n",
    "for col in ohe_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"  - {col}: {df_encoded[col].nunique()} unique values\")\n",
    "        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "        df_encoded = pd.concat([df_encoded.drop(col, axis=1), dummies], axis=1)\n",
    "        print(f\"    â†’ Created {len(dummies.columns)} dummy columns\")\n",
    "\n",
    "# Ordinal Encoding for ordered variables\n",
    "ordinal_mappings = {\n",
    "    'age_group': ['18-30', '31-45', '46-60', '61-75', '76+'],\n",
    "    'income_bracket': ['Missing', 'Low', 'Medium', 'High', 'Very High'],\n",
    "    'tenure_group': ['0-6 months', '6-12 months', '1-2 years', '2+ years']\n",
    "}\n",
    "\n",
    "print(f\"\\nOrdinal Encoding: {list(ordinal_mappings.keys())}\")\n",
    "\n",
    "for col, categories in ordinal_mappings.items():\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"  - {col}: {categories}\")\n",
    "        df_encoded[col + '_encoded'] = df_encoded[col].map({cat: i for i, cat in enumerate(categories)})\n",
    "        df_encoded = df_encoded.drop(col, axis=1)\n",
    "        print(f\"    â†’ Created {col}_encoded (0-{len(categories)-1})\")\n",
    "\n",
    "print(f\"\\nAfter encoding: {df_encoded.shape[0]:,} rows Ã— {df_encoded.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imputation_strategy",
   "metadata": {},
   "source": [
    "## Missing Value Imputation Strategy\n",
    "\n",
    "### Problem Identified\n",
    "After ordinal encoding, categorical features with missing values produce `NaN` in the encoded numeric representation.\n",
    "- **Affected Feature**: `tenure_group_encoded` (111 rows with missing values)\n",
    "- **Root Cause**: `pd.Series.map()` returns `NaN` for unmapped values (missing data)\n",
    "\n",
    "### Imputation Strategy & Justification\n",
    "\n",
    "**Selected Strategy: Median Imputation**\n",
    "\n",
    "**Why Median over Mean?**\n",
    "1. **Robust to outliers** - Median is less affected by extreme values\n",
    "2. **Preserves distribution** - More representative of central tendency when data is skewed\n",
    "3. **Ordinal scale compatibility** - For encoded categorical data (0-3 scale), median maintains reasonable position\n",
    "\n",
    "**Why not Mean?**\n",
    "- Mean could result in non-integer values (e.g., 1.7) on discrete ordinal scales\n",
    "- More sensitive to outliers that could skew the imputed value\n",
    "\n",
    "**Why not Mode?**\n",
    "- Could over-represent the most common tenure group\n",
    "- May lose variance in the dataset\n",
    "- For `tenure_group`: mode = \"6-12 months\" (30%), but median better captures middle of distribution\n",
    "\n",
    "**Why not Remove Rows?**\n",
    "- 111 rows = 0.13% of 86,432 total samples\n",
    "- Removing would lose valuable information from these customers\n",
    "- Could introduce bias if missingness is not random\n",
    "- **Imputation preserves sample size and statistical power**\n",
    "\n",
    "### Data Loss Impact\n",
    "\n",
    "- **Before imputation**: 111/86,432 rows (0.13%) with missing `tenure_group`\n",
    "- **After imputation**: 0 rows with missing values\n",
    "- **Preserved**: All 86,432 samples for training\n",
    "\n",
    "This is acceptable given:\n",
    "- Small percentage of missing data (<1%)\n",
    "- Median provides a reasonable central tendency estimate\n",
    "- The model can still learn from all other features of these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (86432, 24)\n",
      "Target shape: (86432,)\n",
      "\n",
      "Train set: 69,145 samples (80%)\n",
      "Test set: 17,287 samples (20%)\n",
      "\n",
      "Target distribution in train set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop('target', axis=1)\n",
    "y = df_encoded['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]:,} samples ({(1-0.2)*100:.0f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({0.2*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nTarget distribution in train set:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handle_nan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: MISSING VALUE IMPUTATION\n",
      "============================================================\n",
      "\n",
      "Columns with NaN values: 1\n",
      "  - tenure_group_encoded: 87 missing (0.13%)\n",
      "\n",
      "Imputation Strategy: Median Imputation\n",
      "Reason: Robust to outliers, preserves ordinal scale\n",
      "\n",
      "  âœ“ tenure_group_encoded: 87 values imputed with median=2.00\n",
      "\n",
      "============================================================\n",
      "IMPUTATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final Check:\n",
      "  NaN in train: 0\n",
      "  NaN in test: 0\n",
      "  âœ“ All missing values resolved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mx/yldf69cd47dbgrqxh59l6v1c0000gn/T/ipykernel_11679/663433204.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(col_median, inplace=True)\n",
      "/var/folders/mx/yldf69cd47dbgrqxh59l6v1c0000gn/T/ipykernel_11679/663433204.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(col_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 2: MISSING VALUE IMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_cols = X_train.columns[X_train.isnull().any()].tolist()\n",
    "\n",
    "print(f\"\\nColumns with NaN values: {len(nan_cols)}\")\n",
    "if len(nan_cols) > 0:\n",
    "    for col in nan_cols:\n",
    "        missing_count = X_train[col].isnull().sum()\n",
    "        pct = (missing_count / len(X_train)) * 100\n",
    "        print(f\"  - {col}: {missing_count:,} missing ({pct:.2f}%)\")\n",
    "\n",
    "# Impute NaN with median for numerical features\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"\\nImputation Strategy: Median Imputation\")\n",
    "    print(f\"Reason: Robust to outliers, preserves ordinal scale\\n\")\n",
    "    \n",
    "    for col in nan_cols:\n",
    "        missing_count = X_train[col].isnull().sum()\n",
    "        \n",
    "        # Calculate median of non-missing values\n",
    "        col_median = X_train[col][~X_train[col].isnull()].median()\n",
    "        \n",
    "        # Impute in both train and test\n",
    "        X_train[col].fillna(col_median, inplace=True)\n",
    "        X_test[col].fillna(col_median, inplace=True)\n",
    "        \n",
    "        print(f\"  âœ“ {col}: {missing_count} values imputed with median={col_median:.2f}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No NaN values requiring imputation!\")\n",
    "\n",
    "# Final verification\n",
    "final_nan_train = X_train.isnull().sum().sum()\n",
    "final_nan_test = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPUTATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Check:\")\n",
    "print(f\"  NaN in train: {final_nan_train}\")\n",
    "print(f\"  NaN in test: {final_nan_test}\")\n",
    "print(f\"  âœ“ All missing values resolved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identify_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE CATEGORIZATION FOR SCALING\n",
      "============================================================\n",
      "\n",
      "Binary features (not scaling): 6\n",
      "  - in_email\n",
      "  - in_mobile\n",
      "  - in_social\n",
      "  - in_web\n",
      "  - offer_received\n",
      "  - is_demographics_missing\n",
      "\n",
      "Numerical features to scale: 11\n",
      "  - received_time\n",
      "  - difficulty\n",
      "  - duration\n",
      "  - age\n",
      "  - income\n",
      "  - membership_year\n",
      "  - membership_duration_days\n",
      "  - membership_month\n",
      "  - age_group_encoded\n",
      "  - income_bracket_encoded\n",
      "  - tenure_group_encoded\n",
      "\n",
      "âœ“ Total features: 17\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns to scale\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Exclude binary columns (0/1) from scaling\n",
    "numerical_cols_to_scale = []\n",
    "binary_cols = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    unique_vals = X_train[col].nunique()\n",
    "    if unique_vals > 2:  # Not binary\n",
    "        numerical_cols_to_scale.append(col)\n",
    "    else:\n",
    "        binary_cols.append(col)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE CATEGORIZATION FOR SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nBinary features (not scaling): {len(binary_cols)}\")\n",
    "for col in binary_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nNumerical features to scale: {len(numerical_cols_to_scale)}\")\n",
    "for col in numerical_cols_to_scale:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "print(f\"\\nâœ“ Total features: {len(binary_cols) + len(numerical_cols_to_scale)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scaling_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "============================================================\n",
      "\n",
      "Scaled features (sample from train set):\n",
      "       received_time  difficulty  duration       age    income  \\\n",
      "count       69145.00    69145.00  69145.00  69145.00  69145.00   \n",
      "mean           -0.00        0.00      0.00      0.00     -0.00   \n",
      "std             1.00        1.00      1.00      1.00      1.00   \n",
      "min            -1.70       -1.46     -1.66     -1.69     -1.99   \n",
      "25%            -0.84       -0.54     -0.74     -0.65     -0.56   \n",
      "50%             0.38        0.39      0.18     -0.15      0.06   \n",
      "75%             0.87        0.39      0.18      0.39      0.67   \n",
      "max             1.24        2.24      1.56      2.16      2.10   \n",
      "\n",
      "       membership_year  membership_duration_days  membership_month  \\\n",
      "count         69145.00                  69145.00          69145.00   \n",
      "mean              0.00                      0.00              0.00   \n",
      "std               1.00                      1.00              1.00   \n",
      "min              -3.09                     -1.29             -1.63   \n",
      "25%              -0.52                     -0.76             -0.77   \n",
      "50%               0.34                     -0.36              0.09   \n",
      "75%               0.34                      0.68              0.95   \n",
      "max               1.19                      3.17              1.52   \n",
      "\n",
      "       age_group_encoded  income_bracket_encoded  tenure_group_encoded  \n",
      "count           69145.00                69145.00              69145.00  \n",
      "mean                0.00                   -0.00                  0.00  \n",
      "std                 1.00                    1.00                  1.00  \n",
      "min                -1.86                   -1.86                 -1.47  \n",
      "25%                -1.05                   -0.28                 -0.56  \n",
      "50%                -0.25                   -0.28                  0.35  \n",
      "75%                 0.55                    0.50                  1.26  \n",
      "max                 1.36                    1.29                  1.26  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train[numerical_cols_to_scale])\n",
    "\n",
    "# Scale test data (using fitted scaler)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"\\nScaled features (sample from train set):\")\n",
    "print(X_train_scaled[numerical_cols_to_scale].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Original features: 24\n",
      "\n",
      "Feature breakdown:\n",
      "  - Binary features: 6\n",
      "  - Scaled numerical: 11\n",
      "  - One-hot encoded: 7\n",
      "  - Ordinal encoded: 3\n",
      "\n",
      "Imputation Summary:\n",
      "  - Missing values handled: 1 columns\n",
      "  - Strategy: Median imputation\n",
      "  - Samples preserved: 69,145 (100%)\n",
      "  - No data loss through row deletion\n",
      "\n",
      "Final shapes:\n",
      "  X_train_scaled: (69145, 24)\n",
      "  X_test_scaled: (17287, 24)\n",
      "  y_train: (69145,)\n",
      "  y_test: (17287,)\n",
      "\n",
      "Data Quality Checks:\n",
      "  âœ“ No missing values (NaN): True\n",
      "  âœ“ All features numeric: False\n",
      "  âœ“ Target is integer: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Binary features: {len(binary_cols)}\")\n",
    "print(f\"  - Scaled numerical: {len(numerical_cols_to_scale)}\")\n",
    "print(f\"  - One-hot encoded: {len([col for col in X_train.columns if col.startswith('offer_type_') or col.startswith('gender_')])}\")\n",
    "print(f\"  - Ordinal encoded: {len([col for col in X_train.columns if '_encoded' in col])}\")\n",
    "\n",
    "print(f\"\\nImputation Summary:\")\n",
    "print(f\"  - Missing values handled: {len(nan_cols)} columns\")\n",
    "print(f\"  - Strategy: Median imputation\")\n",
    "print(f\"  - Samples preserved: {X_train.shape[0]:,} (100%)\")\n",
    "print(f\"  - No data loss through row deletion\")\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nData Quality Checks:\")\n",
    "print(f\"  âœ“ No missing values (NaN): {X_train_scaled.isnull().sum().sum() == 0}\")\n",
    "print(f\"  âœ“ All features numeric: {all(X_train_scaled.dtypes.apply(lambda x: str(x).startswith(('int', 'float'))))}\")\n",
    "print(f\"  âœ“ Target is integer: {y_train.dtype == 'int64'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_processed",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save processed datasets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ“ PROCESSED DATA SAVED\n",
      "============================================================\n",
      "\n",
      "Saved files:\n",
      "  - X_train_scaled.pkl ((69145, 24))\n",
      "  - X_test_scaled.pkl ((17287, 24))\n",
      "  - y_train.pkl ((69145,))\n",
      "  - y_test.pkl ((17287,))\n",
      "  - scaler.pkl\n",
      "  - feature_names.pkl (24 features)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../Cafe_Rewards_Offers/processed', exist_ok=True)\n",
    "\n",
    "# Save all processed datasets\n",
    "joblib.dump(X_train_scaled, '../Cafe_Rewards_Offers/processed/X_train_scaled.pkl')\n",
    "joblib.dump(X_test_scaled, '../Cafe_Rewards_Offers/processed/X_test_scaled.pkl')\n",
    "joblib.dump(y_train, '../Cafe_Rewards_Offers/processed/y_train.pkl')\n",
    "joblib.dump(y_test, '../Cafe_Rewards_Offers/processed/y_test.pkl')\n",
    "joblib.dump(scaler, '../Cafe_Rewards_Offers/processed/scaler.pkl')\n",
    "\n",
    "# Also save column names for reference\n",
    "feature_names = X_train_scaled.columns.tolist()\n",
    "joblib.dump(feature_names, '../Cafe_Rewards_Offers/processed/feature_names.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ“ PROCESSED DATA SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  - X_train_scaled.pkl ({X_train_scaled.shape})\")\n",
    "print(f\"  - X_test_scaled.pkl ({X_test_scaled.shape})\")\n",
    "print(f\"  - y_train.pkl ({y_train.shape})\")\n",
    "print(f\"  - y_test.pkl ({y_test.shape})\")\n",
    "print(f\"  - scaler.pkl\")\n",
    "print(f\"  - feature_names.pkl ({len(feature_names)} features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_documentation",
   "metadata": {},
   "source": [
    "## Feature Engineering Documentation Summary\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw Data (86KÃ—28) â†’ Drop Leakage (86KÃ—20) â†’ Encode (86KÃ—25) â†’ Split & Drop Target (X: 86KÃ—24) â†’ Impute â†’ Scale â†’ Ready for ML\n",
    "```\n",
    "\n",
    "### Decisions Made & Justifications\n",
    "\n",
    "#### 1. Data Leakage Removal âœ…\n",
    "\n",
    "| Column | Action | Reason |\n",
    "|--------|--------|--------|\n",
    "| `index` | Dropped | Row identifier, no predictive power |\n",
    "| `customer_id` | Dropped | Identifier, doesn't influence offer completion |\n",
    "| `offer_id` | Dropped | Specific offer ID, model should learn characteristics not IDs |\n",
    "| `completion_time` | Dropped | âš ï¸ **DATA LEAKAGE** - Only exists for completed offers |\n",
    "| `time_to_action` | Dropped | âš ï¸ **DATA LEAKAGE** - Only exists for completed offers |\n",
    "| `offer_completed` | Dropped | âš ï¸ **PERFECT LEAKAGE** - This IS the target variable (r=1.0) |\n",
    "| `offer_viewed` | Dropped | âš ï¸ **TEMPORAL LEAKAGE** - Not available at prediction time |\n",
    "| `became_member_on` | Dropped | Raw date format, replaced with derived features |\n",
    "| `became_member_date` | Dropped | Redundant, already have membership features |\n",
    "\n",
    "**âœ… RESULT: Clean dataset with 24 features - all available at offer delivery time**\n",
    "\n",
    "#### 2. Categorical Encoding\n",
    "\n",
    "**One-Hot Encoding (Nominal Features)**:\n",
    "- `offer_type` â†’ 3 binary columns (bogo, discount, informational)\n",
    "- `gender` â†’ 4 binary columns (F, M, O, Missing)\n",
    "- **Why**: No inherent ordering, each category independent\n",
    "- **Decision**: `drop_first=False` to keep all categories (useful for feature importance)\n",
    "\n",
    "**Ordinal Encoding (Ordinal Features)**:\n",
    "- `age_group` â†’ 0 to 4 (18-30 to 76+) â†’ `age_group_encoded`\n",
    "- `income_bracket` â†’ 0 to 4 (Missing to Very High) â†’ `income_bracket_encoded`\n",
    "- `tenure_group` â†’ 0 to 3 (0-6m to 2+years) â†’ `tenure_group_encoded`\n",
    "- **Why**: Preserves natural ordering relationship\n",
    "- **Decision**: `Missing` in income mapped to 0 (lowest tier)\n",
    "\n",
    "#### 3. Missing Value Imputation\n",
    "\n",
    "**Problem**: 87 rows (0.13%) in training set with missing `tenure_group_encoded`\n",
    "\n",
    "**Strategy**: Median imputation (median = 2.0)\n",
    "\n",
    "**Justification**:\n",
    "1. **Robust to outliers** - Less affected by extreme values\n",
    "2. **Preserves ordinal scale** - Works with discrete 0-3 encoding (median=2 maps to \"1-2 years\")\n",
    "3. **Retains data** - No sample loss (preserves all 86,432 samples)\n",
    "4. **Train-test consistency** - Same median value applied to both sets\n",
    "\n",
    "#### 4. Feature Scaling\n",
    "\n",
    "**Method**: StandardScaler (Z-score normalization)\n",
    "- Scales to: mean=0, std=1\n",
    "- **Applied to**: 11 numerical features with >2 unique values\n",
    "- **Not applied to**: Binary features (already 0/1 scale), one-hot encoded features\n",
    "\n",
    "**Features Scaled** (11 total):\n",
    "1. `received_time` - Time offer was received\n",
    "2. `difficulty` - Offer difficulty level\n",
    "3. `duration` - Offer duration in days\n",
    "4. `age` - Customer age\n",
    "5. `income` - Customer income\n",
    "6. `membership_year` - Year customer joined\n",
    "7. `membership_duration_days` - Days since membership started\n",
    "8. `membership_month` - Month customer joined\n",
    "9. `age_group_encoded` - Ordinal encoded age group (0-4)\n",
    "10. `income_bracket_encoded` - Ordinal encoded income (0-4)\n",
    "11. `tenure_group_encoded` - Ordinal encoded tenure (0-3)\n",
    "\n",
    "**Why StandardScaler**:\n",
    "1. Required for Logistic Regression (assumes standardized features)\n",
    "2. Helps gradient descent converge faster\n",
    "3. Makes regularization penalties fair across features\n",
    "4. Preserves interpretability with mean=0, std=1\n",
    "\n",
    "### Final Dataset Statistics\n",
    "\n",
    "- **Total Samples**: 86,432 (100% preserved after imputation)\n",
    "- **Train Split**: 69,145 (80%)\n",
    "- **Test Split**: 17,287 (20%)\n",
    "- **Stratified**: Yes (preserves target class balance)\n",
    "- **Final Features**: **24** (after dropping leakage features and target)\n",
    "  - Binary features: 6 (channel flags, demographics flags)\n",
    "  - One-hot encoded: 7 (3 offer types + 4 gender categories)\n",
    "  - Ordinal encoded: 3 (age_group, income_bracket, tenure_group)\n",
    "  - Scaled numerical: 11 (raw + encoded features)\n",
    "- **Target Balance**: \n",
    "  - Class 1 (completed): 53.4%\n",
    "  - Class 0 (not completed): 46.6%\n",
    "  - Slight imbalance but acceptable (no resampling needed)\n",
    "\n",
    "### Assumptions & Limitations\n",
    "\n",
    "**Assumptions**:\n",
    "1. Missing values in `tenure_group` are random (MCAR - Missing Completely At Random)\n",
    "2. Median is representative of missing values for tenure\n",
    "3. Ordinal scales reflect true ordering (equal intervals assumed)\n",
    "4. No temporal leakage in data split (stratified by target only, not by time)\n",
    "\n",
    "**Limitations**:\n",
    "1. **Imputation bias**: Median may not represent true missing values\n",
    "2. **Variance reduction**: Imputation reduces natural variability in tenure\n",
    "3. **Ordinal encoding assumption**: Equal distance between categories assumed\n",
    "4. **Multicollinearity**: Some features may be correlated (e.g., age and age_group_encoded)\n",
    "\n",
    "### Data Quality Validation\n",
    "\n",
    "**âœ… No Data Leakage:**\n",
    "- Removed `offer_completed` (perfect correlation with target)\n",
    "- Removed `offer_viewed` (temporal leak)\n",
    "- All 24 features available at prediction time\n",
    "\n",
    "**âœ… Clean Dataset:**\n",
    "- No missing values (100% complete after imputation)\n",
    "- All features numeric and properly scaled\n",
    "- Train-test split maintains class balance\n",
    "- Ready for machine learning\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. âœ… Load processed data from `../Cafe_Rewards_Offers/processed/` folder\n",
    "2. âœ… Train baseline models (Logistic Regression, Decision Tree)\n",
    "3. âœ… Train ensemble models (Random Forest, XGBoost)\n",
    "4. âœ… Compare metrics and select best model\n",
    "5. âœ… Perform hyperparameter tuning\n",
    "6. âœ… Apply PCA for dimensionality reduction\n",
    "7. ðŸ”„ Conduct SHAP analysis for model explainability\n",
    "8. ðŸ”„ Perform bias and fairness analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef9755",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_beansage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
