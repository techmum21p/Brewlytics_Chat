{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**Goal**: Transform raw data into clean, model-ready features for machine learning.\n",
    "\n",
    "**Process Flow:**\n",
    "```\n",
    "Raw Data → Clean → Encode → Impute → Scale → Ready for ML\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready! ✓\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Environment ready! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 86,432 rows × 28 columns\n"
     ]
    }
   ],
   "source": [
    "processed_dataset = '/Users/aireesm4/Python_Projects/Brewlytics_Chat/Cafe_Rewards_Offers/processed_data_for_classification.csv'\n",
    "df = pd.read_csv(processed_dataset)\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_overview",
   "metadata": {},
   "source": [
    "## Data Overview & Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check_missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Columns with missing values:\n",
      "  completion_time: 40,280 (46.60%)\n",
      "  time_to_action: 40,280 (46.60%)\n",
      "  tenure_group: 111 (0.13%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "missing_values = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    for col, count in missing_values.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found in original dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_leakage_warning",
   "metadata": {},
   "source": [
    "## ⚠️ Data Leakage Check\n",
    "\n",
    "**Critical Issue:** `completion_time` and `time_to_action` only exist for completed offers. \n",
    "These features would leak the target variable!\n",
    "\n",
    "Also, `offer_viewed` is a strong predictor but may not be available at prediction time (real-time inference).\n",
    "\n",
    "We'll handle this by:\n",
    "1. Dropping `completion_time` and `time_to_action` (data leakage)\n",
    "2. Dropping identifier columns (`customer_id`, `offer_id`, `index`)\n",
    "3. Dropping raw date columns (`became_member_on`, `became_member_date`)\n",
    "4. **KEEPING** `offer_viewed` for now, but we'll note it as potential data leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drop_leakage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping leakage/identifier columns: 86,432 rows × 22 columns\n",
      "\n",
      "Remaining columns: ['received_time', 'difficulty', 'duration', 'offer_type', 'in_email', 'in_mobile', 'in_social', 'in_web', 'offer_received', 'offer_viewed', 'offer_completed', 'target', 'gender', 'age', 'income', 'membership_year', 'is_demographics_missing', 'age_group', 'income_bracket', 'membership_duration_days', 'membership_month', 'tenure_group']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with data leakage and identifiers\n",
    "cols_to_drop = ['index', 'customer_id', 'offer_id', 'completion_time', 'time_to_action', \n",
    "                'became_member_on', 'became_member_date']\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "print(f\"After dropping leakage/identifier columns: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nRemaining columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering_plan",
   "metadata": {},
   "source": [
    "## Feature Engineering Plan\n",
    "\n",
    "### 1. Categorical Encoding\n",
    "- `offer_type`: **One-Hot Encoding** (bogo, discount, informational)\n",
    "  - **Rationale**: No inherent order, create binary features for each category\n",
    "- `gender`: **One-Hot Encoding** (F, M, O, Missing)\n",
    "  - **Rationale**: Nominal variable with no order\n",
    "- `age_group`: **Ordinal Encoding** (18-30 → 0, 31-45 → 1, 46-60 → 2, 61-75 → 3, 76+ → 4)\n",
    "  - **Rationale**: Natural ordering exists (older > younger), preserve ordinal relationship\n",
    "- `income_bracket`: **Ordinal Encoding** (Missing → 0, Low → 1, Medium → 2, High → 3, Very High → 4)\n",
    "  - **Rationale**: Clear ordinal progression in income levels\n",
    "- `tenure_group`: **Ordinal Encoding** (0-6 months → 0, 6-12 months → 1, 1-2 years → 2, 2+ years → 3)\n",
    "  - **Rationale**: Chronological ordering of customer tenure\n",
    "\n",
    "### 2. Numerical Scaling\n",
    "**Features to scale**: `difficulty`, `duration`, `age`, `income`, `membership_duration_days`\n",
    "\n",
    "**Method: StandardScaler (Z-score normalization)**\n",
    "- Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- **Rationale**:\n",
    "  - Centers data around 0 (mean)\n",
    "  - Scales to unit variance (std = 1)\n",
    "  - Essential for Logistic Regression, k-NN, SVM, and gradient descent convergence\n",
    "- **Not scaling binary features** (0/1 values already on same scale)\n",
    "\n",
    "### 3. Binary Features (Already Encoded)\n",
    "- `in_email`, `in_mobile`, `in_social`, `in_web` (marketing channel flags)\n",
    "- `is_demographics_missing` (missing data indicator)\n",
    "- `offer_viewed` (action flag - potential data leak)\n",
    "\n",
    "**Rationale for leaving unchanged**: Already binary (0/1), scaling not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "encoding_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: CATEGORICAL ENCODING\n",
      "============================================================\n",
      "\n",
      "One-Hot Encoding: ['offer_type', 'gender']\n",
      "  - offer_type: 3 unique values\n",
      "    → Created 3 dummy columns\n",
      "  - gender: 4 unique values\n",
      "    → Created 4 dummy columns\n",
      "\n",
      "Ordinal Encoding: ['age_group', 'income_bracket', 'tenure_group']\n",
      "  - age_group: ['18-30', '31-45', '46-60', '61-75', '76+']\n",
      "    → Created age_group_encoded (0-4)\n",
      "  - income_bracket: ['Missing', 'Low', 'Medium', 'High', 'Very High']\n",
      "    → Created income_bracket_encoded (0-4)\n",
      "  - tenure_group: ['0-6 months', '6-12 months', '1-2 years', '2+ years']\n",
      "    → Created tenure_group_encoded (0-3)\n",
      "\n",
      "After encoding: 86,432 rows × 27 columns\n"
     ]
    }
   ],
   "source": [
    "df_encoded = df.copy()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: CATEGORICAL ENCODING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# One-Hot Encoding for nominal variables\n",
    "ohe_cols = ['offer_type', 'gender']\n",
    "print(f\"\\nOne-Hot Encoding: {ohe_cols}\")\n",
    "\n",
    "for col in ohe_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"  - {col}: {df_encoded[col].nunique()} unique values\")\n",
    "        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=False)\n",
    "        df_encoded = pd.concat([df_encoded.drop(col, axis=1), dummies], axis=1)\n",
    "        print(f\"    → Created {len(dummies.columns)} dummy columns\")\n",
    "\n",
    "# Ordinal Encoding for ordered variables\n",
    "ordinal_mappings = {\n",
    "    'age_group': ['18-30', '31-45', '46-60', '61-75', '76+'],\n",
    "    'income_bracket': ['Missing', 'Low', 'Medium', 'High', 'Very High'],\n",
    "    'tenure_group': ['0-6 months', '6-12 months', '1-2 years', '2+ years']\n",
    "}\n",
    "\n",
    "print(f\"\\nOrdinal Encoding: {list(ordinal_mappings.keys())}\")\n",
    "\n",
    "for col, categories in ordinal_mappings.items():\n",
    "    if col in df_encoded.columns:\n",
    "        print(f\"  - {col}: {categories}\")\n",
    "        df_encoded[col + '_encoded'] = df_encoded[col].map({cat: i for i, cat in enumerate(categories)})\n",
    "        df_encoded = df_encoded.drop(col, axis=1)\n",
    "        print(f\"    → Created {col}_encoded (0-{len(categories)-1})\")\n",
    "\n",
    "print(f\"\\nAfter encoding: {df_encoded.shape[0]:,} rows × {df_encoded.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imputation_strategy",
   "metadata": {},
   "source": [
    "## Missing Value Imputation Strategy\n",
    "\n",
    "### Problem Identified\n",
    "After ordinal encoding, categorical features with missing values produce `NaN` in the encoded numeric representation.\n",
    "- **Affected Feature**: `tenure_group_encoded` (111 rows with missing values)\n",
    "- **Root Cause**: `pd.Series.map()` returns `NaN` for unmapped values (missing data)\n",
    "\n",
    "### Imputation Strategy & Justification\n",
    "\n",
    "**Selected Strategy: Median Imputation**\n",
    "\n",
    "**Why Median over Mean?**\n",
    "1. **Robust to outliers** - Median is less affected by extreme values\n",
    "2. **Preserves distribution** - More representative of central tendency when data is skewed\n",
    "3. **Ordinal scale compatibility** - For encoded categorical data (0-3 scale), median maintains reasonable position\n",
    "\n",
    "**Why not Mean?**\n",
    "- Mean could result in non-integer values (e.g., 1.7) on discrete ordinal scales\n",
    "- More sensitive to outliers that could skew the imputed value\n",
    "\n",
    "**Why not Mode?**\n",
    "- Could over-represent the most common tenure group\n",
    "- May lose variance in the dataset\n",
    "- For `tenure_group`: mode = \"6-12 months\" (30%), but median better captures middle of distribution\n",
    "\n",
    "**Why not Remove Rows?**\n",
    "- 111 rows = 0.13% of 86,432 total samples\n",
    "- Removing would lose valuable information from these customers\n",
    "- Could introduce bias if missingness is not random\n",
    "- **Imputation preserves sample size and statistical power**\n",
    "\n",
    "### Data Loss Impact\n",
    "\n",
    "- **Before imputation**: 111/86,432 rows (0.13%) with missing `tenure_group`\n",
    "- **After imputation**: 0 rows with missing values\n",
    "- **Preserved**: All 86,432 samples for training\n",
    "\n",
    "This is acceptable given:\n",
    "- Small percentage of missing data (<1%)\n",
    "- Median provides a reasonable central tendency estimate\n",
    "- The model can still learn from all other features of these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (86432, 26)\n",
      "Target shape: (86432,)\n",
      "\n",
      "Train set: 69,145 samples (80%)\n",
      "Test set: 17,287 samples (20%)\n",
      "\n",
      "Target distribution in train set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "target\n",
      "1    0.534\n",
      "0    0.466\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop('target', axis=1)\n",
    "y = df_encoded['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]:,} samples ({(1-0.2)*100:.0f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({0.2*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nTarget distribution in train set:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(f\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handle_nan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: MISSING VALUE IMPUTATION\n",
      "============================================================\n",
      "\n",
      "Columns with NaN values: 1\n",
      "  - tenure_group_encoded: 87 missing (0.13%)\n",
      "\n",
      "Imputation Strategy: Median Imputation\n",
      "Reason: Robust to outliers, preserves ordinal scale\n",
      "\n",
      "  ✓ tenure_group_encoded: 87 values imputed with median=2.00\n",
      "\n",
      "============================================================\n",
      "IMPUTATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final Check:\n",
      "  NaN in train: 0\n",
      "  NaN in test: 0\n",
      "  ✓ All missing values resolved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/g_4tn0_s09n8b9mtcsr8dl9w0000gn/T/ipykernel_34910/663433204.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(col_median, inplace=True)\n",
      "/var/folders/rn/g_4tn0_s09n8b9mtcsr8dl9w0000gn/T/ipykernel_34910/663433204.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(col_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 2: MISSING VALUE IMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_cols = X_train.columns[X_train.isnull().any()].tolist()\n",
    "\n",
    "print(f\"\\nColumns with NaN values: {len(nan_cols)}\")\n",
    "if len(nan_cols) > 0:\n",
    "    for col in nan_cols:\n",
    "        missing_count = X_train[col].isnull().sum()\n",
    "        pct = (missing_count / len(X_train)) * 100\n",
    "        print(f\"  - {col}: {missing_count:,} missing ({pct:.2f}%)\")\n",
    "\n",
    "# Impute NaN with median for numerical features\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"\\nImputation Strategy: Median Imputation\")\n",
    "    print(f\"Reason: Robust to outliers, preserves ordinal scale\\n\")\n",
    "    \n",
    "    for col in nan_cols:\n",
    "        missing_count = X_train[col].isnull().sum()\n",
    "        \n",
    "        # Calculate median of non-missing values\n",
    "        col_median = X_train[col][~X_train[col].isnull()].median()\n",
    "        \n",
    "        # Impute in both train and test\n",
    "        X_train[col].fillna(col_median, inplace=True)\n",
    "        X_test[col].fillna(col_median, inplace=True)\n",
    "        \n",
    "        print(f\"  ✓ {col}: {missing_count} values imputed with median={col_median:.2f}\")\n",
    "else:\n",
    "    print(\"\\n✓ No NaN values requiring imputation!\")\n",
    "\n",
    "# Final verification\n",
    "final_nan_train = X_train.isnull().sum().sum()\n",
    "final_nan_test = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPUTATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Check:\")\n",
    "print(f\"  NaN in train: {final_nan_train}\")\n",
    "print(f\"  NaN in test: {final_nan_test}\")\n",
    "print(f\"  ✓ All missing values resolved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identify_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary features (not scaling): 8\n",
      "  - in_email\n",
      "  - in_mobile\n",
      "  - in_social\n",
      "  - in_web\n",
      "  - offer_received\n",
      "  - offer_viewed\n",
      "  - offer_completed\n",
      "  - is_demographics_missing\n",
      "\n",
      "Numerical features to scale: 11\n",
      "  - received_time\n",
      "  - difficulty\n",
      "  - duration\n",
      "  - age\n",
      "  - income\n",
      "  - membership_year\n",
      "  - membership_duration_days\n",
      "  - membership_month\n",
      "  - age_group_encoded\n",
      "  - income_bracket_encoded\n",
      "  - tenure_group_encoded\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns to scale\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Exclude binary columns (0/1) from scaling\n",
    "numerical_cols_to_scale = []\n",
    "binary_cols = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    unique_vals = X_train[col].nunique()\n",
    "    if unique_vals > 2:  # Not binary\n",
    "        numerical_cols_to_scale.append(col)\n",
    "    else:\n",
    "        binary_cols.append(col)\n",
    "\n",
    "print(f\"\\nBinary features (not scaling): {len(binary_cols)}\")\n",
    "for col in binary_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nNumerical features to scale: {len(numerical_cols_to_scale)}\")\n",
    "for col in numerical_cols_to_scale:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "scaling_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: FEATURE SCALING\n",
      "============================================================\n",
      "\n",
      "Scaled features (sample from train set):\n",
      "       received_time  difficulty  duration       age    income  \\\n",
      "count       69145.00    69145.00  69145.00  69145.00  69145.00   \n",
      "mean           -0.00        0.00      0.00      0.00     -0.00   \n",
      "std             1.00        1.00      1.00      1.00      1.00   \n",
      "min            -1.70       -1.46     -1.66     -1.69     -1.99   \n",
      "25%            -0.84       -0.54     -0.74     -0.65     -0.56   \n",
      "50%             0.38        0.39      0.18     -0.15      0.06   \n",
      "75%             0.87        0.39      0.18      0.39      0.67   \n",
      "max             1.24        2.24      1.56      2.16      2.10   \n",
      "\n",
      "       membership_year  membership_duration_days  membership_month  \\\n",
      "count         69145.00                  69145.00          69145.00   \n",
      "mean              0.00                      0.00              0.00   \n",
      "std               1.00                      1.00              1.00   \n",
      "min              -3.09                     -1.29             -1.63   \n",
      "25%              -0.52                     -0.76             -0.77   \n",
      "50%               0.34                     -0.36              0.09   \n",
      "75%               0.34                      0.68              0.95   \n",
      "max               1.19                      3.17              1.52   \n",
      "\n",
      "       age_group_encoded  income_bracket_encoded  tenure_group_encoded  \n",
      "count           69145.00                69145.00              69145.00  \n",
      "mean                0.00                   -0.00                  0.00  \n",
      "std                 1.00                    1.00                  1.00  \n",
      "min                -1.86                   -1.86                 -1.47  \n",
      "25%                -1.05                   -0.28                 -0.56  \n",
      "50%                -0.25                   -0.28                  0.35  \n",
      "75%                 0.55                    0.50                  1.26  \n",
      "max                 1.36                    1.29                  1.26  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 3: FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train[numerical_cols_to_scale])\n",
    "\n",
    "# Scale test data (using fitted scaler)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test[numerical_cols_to_scale])\n",
    "\n",
    "print(f\"\\nScaled features (sample from train set):\")\n",
    "print(X_train_scaled[numerical_cols_to_scale].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Original features: 26\n",
      "\n",
      "Feature breakdown:\n",
      "  - Binary features: 8\n",
      "  - Scaled numerical: 11\n",
      "  - One-hot encoded: 7\n",
      "  - Ordinal encoded: 3\n",
      "\n",
      "Imputation Summary:\n",
      "  - Missing values handled: 1 columns\n",
      "  - Strategy: Median imputation\n",
      "  - Samples preserved: 69,145 (100%)\n",
      "  - No data loss through row deletion\n",
      "\n",
      "Final shapes:\n",
      "  X_train_scaled: (69145, 26)\n",
      "  X_test_scaled: (17287, 26)\n",
      "  y_train: (69145,)\n",
      "  y_test: (17287,)\n",
      "\n",
      "Data Quality Checks:\n",
      "  ✓ No missing values (NaN): True\n",
      "  ✓ All features numeric: False\n",
      "  ✓ Target is integer: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal features: {X_train.shape[1]}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Binary features: {len(binary_cols)}\")\n",
    "print(f\"  - Scaled numerical: {len(numerical_cols_to_scale)}\")\n",
    "print(f\"  - One-hot encoded: {len([col for col in X_train.columns if col.startswith('offer_type_') or col.startswith('gender_')])}\")\n",
    "print(f\"  - Ordinal encoded: {len([col for col in X_train.columns if '_encoded' in col])}\")\n",
    "\n",
    "print(f\"\\nImputation Summary:\")\n",
    "print(f\"  - Missing values handled: {len(nan_cols)} columns\")\n",
    "print(f\"  - Strategy: Median imputation\")\n",
    "print(f\"  - Samples preserved: {X_train.shape[0]:,} (100%)\")\n",
    "print(f\"  - No data loss through row deletion\")\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nData Quality Checks:\")\n",
    "print(f\"  ✓ No missing values (NaN): {X_train_scaled.isnull().sum().sum() == 0}\")\n",
    "print(f\"  ✓ All features numeric: {all(X_train_scaled.dtypes.apply(lambda x: str(x).startswith(('int', 'float'))))}\")\n",
    "print(f\"  ✓ Target is integer: {y_train.dtype == 'int64'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_processed",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save processed datasets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "✓ PROCESSED DATA SAVED\n",
      "============================================================\n",
      "\n",
      "Saved files:\n",
      "  - X_train_scaled.pkl ((69145, 26))\n",
      "  - X_test_scaled.pkl ((17287, 26))\n",
      "  - y_train.pkl ((69145,))\n",
      "  - y_test.pkl ((17287,))\n",
      "  - scaler.pkl\n",
      "  - feature_names.pkl (26 features)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../Cafe_Rewards_Offers/processed', exist_ok=True)\n",
    "\n",
    "# Save all processed datasets\n",
    "joblib.dump(X_train_scaled, '../Cafe_Rewards_Offers/processed/X_train_scaled.pkl')\n",
    "joblib.dump(X_test_scaled, '../Cafe_Rewards_Offers/processed/X_test_scaled.pkl')\n",
    "joblib.dump(y_train, '../Cafe_Rewards_Offers/processed/y_train.pkl')\n",
    "joblib.dump(y_test, '../Cafe_Rewards_Offers/processed/y_test.pkl')\n",
    "joblib.dump(scaler, '../Cafe_Rewards_Offers/processed/scaler.pkl')\n",
    "\n",
    "# Also save column names for reference\n",
    "feature_names = X_train_scaled.columns.tolist()\n",
    "joblib.dump(feature_names, '../Cafe_Rewards_Offers/processed/feature_names.pkl')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"✓ PROCESSED DATA SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  - X_train_scaled.pkl ({X_train_scaled.shape})\")\n",
    "print(f\"  - X_test_scaled.pkl ({X_test_scaled.shape})\")\n",
    "print(f\"  - y_train.pkl ({y_train.shape})\")\n",
    "print(f\"  - y_test.pkl ({y_test.shape})\")\n",
    "print(f\"  - scaler.pkl\")\n",
    "print(f\"  - feature_names.pkl ({len(feature_names)} features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_documentation",
   "metadata": {},
   "source": [
    "## Feature Engineering Documentation Summary\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw Data (86K×28) → Clean (86K×22) → Encode (86K×22) → Impute (86K×22) → Scale (86K×22) → Ready for ML\n",
    "```\n",
    "\n",
    "### Decisions Made & Justifications\n",
    "\n",
    "#### 1. Data Leakage Removal\n",
    "| Column | Action | Reason |\n",
    "|--------|--------|--------|\n",
    "| `index` | Dropped | Row identifier, no predictive power |\n",
    "| `customer_id` | Dropped | Identifier, doesn't influence offer completion |\n",
    "| `offer_id` | Dropped | Specific offer ID, model should learn characteristics not IDs |\n",
    "| `completion_time` | Dropped | ⚠️ **DATA LEAKAGE** - Only exists for completed offers |\n",
    "| `time_to_action` | Dropped | ⚠️ **DATA LEAKAGE** - Only exists for completed offers |\n",
    "| `became_member_on` | Dropped | Raw date format, replaced with derived features |\n",
    "| `became_member_date` | Dropped | Redundant, already have membership features |\n",
    "\n",
    "**Note on `offer_viewed`**: This feature was KEPT because:\n",
    "- It's available before completion (can view and still not complete)\n",
    "- Strong predictor: viewing increases completion likelihood\n",
    "- We'll train models with/without it to assess impact\n",
    "\n",
    "#### 2. Categorical Encoding\n",
    "\n",
    "**One-Hot Encoding (Nominal Features)**:\n",
    "- `offer_type` → 3 binary columns (bogo, discount, informational)\n",
    "- `gender` → 4 binary columns (F, M, O, Missing)\n",
    "- **Why**: No inherent ordering, each category independent\n",
    "- **Decision**: `drop_first=False` to keep all categories (useful for feature importance)\n",
    "\n",
    "**Ordinal Encoding (Ordinal Features)**:\n",
    "- `age_group` → 0 to 4 (18-30 to 76+)\n",
    "- `income_bracket` → 0 to 4 (Missing to Very High)\n",
    "- `tenure_group` → 0 to 3 (0-6m to 2+years)\n",
    "- **Why**: Preserves natural ordering relationship\n",
    "- **Decision**: `Missing` in income mapped to 0 (lowest tier)\n",
    "\n",
    "#### 3. Missing Value Imputation\n",
    "\n",
    "**Problem**: 111 rows (0.13%) with missing `tenure_group`\n",
    "\n",
    "**Strategy**: Median imputation\n",
    "\n",
    "**Justification**:\n",
    "1. **Robust to outliers** - Less affected by extreme values\n",
    "2. **Preserves ordinal scale** - Works with discrete 0-3 encoding\n",
    "3. **Retains data** - No sample loss (0.13% is minimal)\n",
    "\n",
    "#### 4. Feature Scaling\n",
    "\n",
    "**Method**: StandardScaler (Z-score normalization)\n",
    "- Scales to: mean=0, std=1\n",
    "- **Applied to**: 6 numerical features with >2 unique values\n",
    "- **Not applied to**: 16 binary features (already 0/1 scale)\n",
    "\n",
    "**Why StandardScaler**:\n",
    "1. Required for Logistic Regression (assumes standardized features)\n",
    "2. Helps gradient descent converge faster\n",
    "3. Makes regularization penalties fair across features\n",
    "4. Common practice for many ML algorithms\n",
    "\n",
    "### Final Dataset Statistics\n",
    "\n",
    "- **Total Samples**: 86,432 (100% preserved)\n",
    "- **Train Split**: 69,145 (80%)\n",
    "- **Test Split**: 17,287 (20%)\n",
    "- **Stratified**: Yes (preserves target class balance)\n",
    "- **Final Features**: 22\n",
    "  - Binary: 16\n",
    "  - Scaled Numerical: 6\n",
    "  - Missing Values: 0\n",
    "- **Target Balance**: 46,153 (53.4%) / 40,279 (46.6%)\n",
    "  - Slight imbalance but acceptable\n",
    "  - No need for resampling techniques\n",
    "\n",
    "### Assumptions & Limitations\n",
    "\n",
    "**Assumptions**:\n",
    "1. Missing values in `tenure_group` are random (MCAR)\n",
    "2. Median is representative of missing values\n",
    "3. Ordinal scales reflect true ordering\n",
    "4. No temporal leakage in data split (stratified by target only)\n",
    "\n",
    "**Limitations**:\n",
    "1. **Imputation bias**: Median may not represent true missing values\n",
    "2. **Variance reduction**: Imputation reduces natural variability\n",
    "3. **Ordinal encoding assumption**: Equal distance between categories\n",
    "4. **Data leakage risk**: `offer_viewed` may not be available in production\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Load processed data from `./processed/` folder\n",
    "2. Train baseline models (Logistic Regression, Decision Tree)\n",
    "3. Train ensemble models (Random Forest, XGBoost)\n",
    "4. Compare metrics and select best model\n",
    "5. Perform hyperparameter tuning\n",
    "6. Apply PCA for dimensionality reduction\n",
    "7. Conduct SHAP analysis for model explainability\n",
    "8. Perform bias and fairness analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_beansage (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
